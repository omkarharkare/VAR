{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install SoccerNet"
      ],
      "metadata": {
        "id": "jiPUWYny4ZT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from SoccerNet.Downloader import SoccerNetDownloader as SNdl\n",
        "\n",
        "# Set up the downloader\n",
        "local_directory = \"path/to/SoccerNet\"\n",
        "mySNdl = SNdl(LocalDirectory=local_directory)\n",
        "\n",
        "# Download the data\n",
        "mySNdl.downloadDataTask(task=\"mvfouls\", split=[\"train\", \"valid\", \"test\", \"challenge\"], password=\"s0cc3rn3t\")\n",
        "\n",
        "# Unzip the downloaded files\n",
        "task_directory = os.path.join(local_directory, \"mvfouls\")\n",
        "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
        "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
        "    if os.path.exists(zip_file):\n",
        "        # Create a new folder with the same name as the zip file\n",
        "        extract_folder = os.path.join(task_directory, split)\n",
        "        os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "        # Extract the contents to the new folder\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_folder)\n",
        "        print(f\"Extracted {split}.zip to {extract_folder}\")\n",
        "    else:\n",
        "        print(f\"{split}.zip not found\")\n",
        "\n",
        "# Optionally, remove the zip files after extraction\n",
        "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
        "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
        "    if os.path.exists(zip_file):\n",
        "        os.remove(zip_file)\n",
        "        print(f\"Removed {split}.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W3UFXnV4bMw",
        "outputId": "24fe5043-ad12-4b68-8fab-74baa0de902d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading path/to/SoccerNet/mvfouls/train.zip...: : 2.46GiB [04:17, 9.55MiB/s]                         \n",
            "Downloading path/to/SoccerNet/mvfouls/valid.zip...: : 351MiB [00:23, 15.0MiB/s]                        \n",
            "Downloading path/to/SoccerNet/mvfouls/test.zip...: : 268MiB [00:18, 14.6MiB/s]                        \n",
            "Downloading path/to/SoccerNet/mvfouls/challenge.zip...: : 246MiB [00:16, 14.8MiB/s]                        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted train.zip to path/to/SoccerNet/mvfouls/train\n",
            "Extracted valid.zip to path/to/SoccerNet/mvfouls/valid\n",
            "Extracted test.zip to path/to/SoccerNet/mvfouls/test\n",
            "Extracted challenge.zip to path/to/SoccerNet/mvfouls/challenge\n",
            "Removed train.zip\n",
            "Removed valid.zip\n",
            "Removed test.zip\n",
            "Removed challenge.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JSpI3G7Y4U38"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set the desired frame count\n",
        "DESIRED_FRAME_COUNT = 126\n",
        "\n",
        "# Load the EVENT_DICTIONARY for mapping annotation labels\n",
        "EVENT_DICTIONARY = {\n",
        "    'action_class': {\"Tackling\": 0, \"Standing tackling\": 1, \"High leg\": 2, \"Holding\": 3, \"Pushing\": 4,\n",
        "                     \"Elbowing\": 5, \"Challenge\": 6, \"Dive\": 7, \"Dont know\": 8},\n",
        "    'offence_class': {\"Offence\": 0, \"Between\": 1, \"No Offence\": 2, \"No offence\": 2},\n",
        "    'severity_class': {\"1.0\": 0, \"2.0\": 1, \"3.0\": 2, \"4.0\": 3, \"5.0\": 4},\n",
        "    'bodypart_class': {\"Upper body\": 0, \"Under body\": 1},\n",
        "    'offence_severity_class': {\"No offence\": 0, \"Offence + No card\": 1, \"Offence + Yellow card\": 2, \"Offence + Red card\": 3}\n",
        "}\n",
        "\n",
        "# Transformation for RGB preprocessing\n",
        "rgb_transform = transforms.Compose([\n",
        "    transforms.Resize((56, 56)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Transformation for flow preprocessing\n",
        "flow_transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def load_filtered_clips_and_labels(DATA_PATH, split, max_samples):\n",
        "    rgb_clips, flow_clips = [], []\n",
        "    labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = [], [], [], [], []\n",
        "\n",
        "    annotations_path = os.path.join(DATA_PATH, split, \"annotations.json\")\n",
        "    print(f\"Loading annotations from: {annotations_path}\")\n",
        "\n",
        "    with open(annotations_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "    print(f\"Total actions found in annotations: {len(annotations['Actions'])}\")\n",
        "\n",
        "    offence_count, no_offence_count, skipped_actions = 0, 0, 0\n",
        "\n",
        "    for action_index, (action_key, action_data) in enumerate(annotations['Actions'].items()):\n",
        "        offence_class = action_data['Offence']\n",
        "        if (offence_class == \"Offence\" and offence_count >= max_samples) or \\\n",
        "           (offence_class in [\"No offence\", \"No Offence\"] and no_offence_count >= max_samples):\n",
        "            continue\n",
        "\n",
        "        # Map labels to indices using the dictionary\n",
        "        action_label = EVENT_DICTIONARY['action_class'].get(action_data['Action class'])\n",
        "        offence_label = EVENT_DICTIONARY['offence_class'].get(offence_class)\n",
        "        severity_label = EVENT_DICTIONARY['severity_class'].get(action_data.get('Severity', '1.0'))\n",
        "        bodypart_label = EVENT_DICTIONARY['bodypart_class'].get(action_data.get('Bodypart', 'Upper body'))\n",
        "        offence_severity = f\"{offence_class} + {EVENT_DICTIONARY['severity_class'].get(severity_label, 'No card')}\"\n",
        "        offence_severity_label = EVENT_DICTIONARY['offence_severity_class'].get(offence_severity, 0)\n",
        "\n",
        "        # Skip if any label is missing\n",
        "        if None in [action_label, offence_label, severity_label, bodypart_label, offence_severity_label]:\n",
        "            skipped_actions += 1\n",
        "            continue\n",
        "\n",
        "        action_folder = os.path.join(DATA_PATH, split, f\"action_{action_key}\")\n",
        "        if not os.path.exists(action_folder):\n",
        "            skipped_actions += 1\n",
        "            continue\n",
        "\n",
        "        rgb_action_clips, flow_action_clips = [], []\n",
        "        for clip_idx in range(2):\n",
        "            clip_path = os.path.join(action_folder, f\"clip_{clip_idx}.mp4\")\n",
        "            if not os.path.exists(clip_path):\n",
        "                continue\n",
        "\n",
        "            cap = cv2.VideoCapture(clip_path)\n",
        "            ret, prev_frame = cap.read()\n",
        "            if not ret:\n",
        "                continue\n",
        "\n",
        "            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "            rgb_frames, flow_frames = [], []\n",
        "\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Process RGB frame\n",
        "                rgb_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "                rgb_frame = rgb_transform(rgb_frame)\n",
        "                rgb_frames.append(rgb_frame)\n",
        "\n",
        "                # Process Optical Flow\n",
        "                curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "                flow = np.clip(flow, -20, 20)  # Clipping to limit extreme values\n",
        "                flow = ((flow + 20) * (255.0 / 40)).astype(np.uint8)  # Normalizing to 0-255 range\n",
        "                flow_frame = Image.fromarray(flow[..., 0])  # Taking the horizontal component for simplicity\n",
        "                flow_frame = flow_transform(flow_frame)\n",
        "                flow_frames.append(flow_frame)\n",
        "                prev_gray = curr_gray\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            # Adjust frame count\n",
        "            if len(rgb_frames) > DESIRED_FRAME_COUNT:\n",
        "                indices = np.linspace(0, len(rgb_frames) - 1, DESIRED_FRAME_COUNT).astype(int)\n",
        "                rgb_frames = [rgb_frames[i] for i in indices]\n",
        "                flow_frames = [flow_frames[i] for i in indices]\n",
        "            elif len(rgb_frames) < DESIRED_FRAME_COUNT:\n",
        "                rgb_frames += [rgb_frames[-1]] * (DESIRED_FRAME_COUNT - len(rgb_frames))\n",
        "                flow_frames += [flow_frames[-1]] * (DESIRED_FRAME_COUNT - len(flow_frames))\n",
        "\n",
        "            rgb_action_clips.append(torch.stack(rgb_frames, dim=0))\n",
        "            flow_action_clips.append(torch.stack(flow_frames, dim=0))\n",
        "\n",
        "        if rgb_action_clips and flow_action_clips:\n",
        "            rgb_clips.append(rgb_action_clips)\n",
        "            flow_clips.append(flow_action_clips)\n",
        "            labels_action.append(action_label)\n",
        "            labels_offence.append(offence_label)\n",
        "            labels_severity.append(severity_label)\n",
        "            labels_bodypart.append(bodypart_label)\n",
        "            labels_offence_severity.append(offence_severity_label)\n",
        "\n",
        "            if offence_class == \"Offence\":\n",
        "                offence_count += 1\n",
        "            else:\n",
        "                no_offence_count += 1\n",
        "\n",
        "        if offence_count >= max_samples and no_offence_count >= max_samples:\n",
        "            break\n",
        "\n",
        "    print(\"\\nSummary:\")\n",
        "    print(f\"Total actions loaded: {len(rgb_clips)}\")\n",
        "    print(f\"Total actions skipped: {skipped_actions}\")\n",
        "    return rgb_clips, flow_clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FSfawep-4U3_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class TwoStreamNetwork(nn.Module):\n",
        "    def __init__(self, num_classes_action=9, num_classes_offence=3, num_classes_severity=5,\n",
        "                 num_classes_bodypart=2, num_classes_offence_severity=4, freeze_backbone=True):\n",
        "        super(TwoStreamNetwork, self).__init__()\n",
        "\n",
        "        # Load the backbone for both streams\n",
        "        self.rgb_backbone = models.resnet50(weights='IMAGENET1K_V1')\n",
        "        self.flow_backbone = models.resnet50(weights='IMAGENET1K_V1')\n",
        "\n",
        "        # Optionally freeze backbone layers\n",
        "        if freeze_backbone:\n",
        "            for param in self.rgb_backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.flow_backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Replace the final layer with Identity for both backbones\n",
        "        num_ftrs = self.rgb_backbone.fc.in_features\n",
        "        self.rgb_backbone.fc = nn.Identity()  # RGB Stream\n",
        "        self.flow_backbone.fc = nn.Identity()  # Optical Flow Stream\n",
        "\n",
        "        # Define fully connected layers for classification\n",
        "        self.fc_action = nn.Linear(num_ftrs * 2, num_classes_action)\n",
        "        self.fc_offence = nn.Linear(num_ftrs * 2, num_classes_offence)\n",
        "        self.fc_severity = nn.Linear(num_ftrs * 2, num_classes_severity)\n",
        "        self.fc_bodypart = nn.Linear(num_ftrs * 2, num_classes_bodypart)\n",
        "        self.fc_offence_severity = nn.Linear(num_ftrs * 2, num_classes_offence_severity)\n",
        "\n",
        "    def forward(self, rgb_input, flow_input):\n",
        "        # Extract batch size and frame count from the RGB input\n",
        "        batch_size, num_streams, num_frames, _, _, _ = rgb_input.shape  # Shape: [batch_size, num_streams, num_frames, 3, 56, 56]\n",
        "\n",
        "        # Reshape input tensors for per-frame processing\n",
        "        rgb_input = rgb_input.view(batch_size * num_streams * num_frames, 3, 112, 112)  # Flatten to [batch_size * num_streams * num_frames, 3, 56, 56]\n",
        "        flow_input = flow_input.view(batch_size * num_streams * num_frames, 1, 112, 112)  # Flatten to [batch_size * num_streams * num_frames, 1, 56, 56]\n",
        "        flow_input = flow_input.repeat(1, 3, 1, 1)  # Repeat the single channel three times\n",
        "\n",
        "        # Process each frame through backbones\n",
        "        rgb_features = self.rgb_backbone(rgb_input)  # Shape: [batch_size * num_streams * num_frames, num_ftrs]\n",
        "        flow_features = self.flow_backbone(flow_input)  # Shape: [batch_size * num_streams * num_frames, num_ftrs]\n",
        "\n",
        "        # Reshape back to [batch_size, num_streams, num_frames, num_ftrs]\n",
        "        rgb_features = rgb_features.view(batch_size, num_streams, num_frames, -1)\n",
        "        flow_features = flow_features.view(batch_size, num_streams, num_frames, -1)\n",
        "\n",
        "        # Aggregate features across frames (mean pooling over frames)\n",
        "        rgb_features = rgb_features.mean(dim=2)  # Shape: [batch_size, num_streams, num_ftrs]\n",
        "        flow_features = flow_features.mean(dim=2)  # Shape: [batch_size, num_streams, num_ftrs]\n",
        "\n",
        "        # Combine features from both streams\n",
        "        combined_features = torch.cat((rgb_features, flow_features), dim=2)  # Shape: [batch_size, num_streams, num_ftrs * 2]\n",
        "\n",
        "        # You may want to aggregate across streams as well, if applicable\n",
        "        combined_features = combined_features.mean(dim=1)  # Optionally, take mean across streams\n",
        "\n",
        "        # Forward through task-specific layers\n",
        "        action_out = self.fc_action(combined_features)\n",
        "        offence_out = self.fc_offence(combined_features)\n",
        "        severity_out = self.fc_severity(combined_features)\n",
        "        bodypart_out = self.fc_bodypart(combined_features)\n",
        "        offence_severity_out = self.fc_offence_severity(combined_features)\n",
        "\n",
        "        return action_out, offence_out, severity_out, bodypart_out, offence_severity_out\n",
        "\n",
        "# # Example of model instantiation\n",
        "# model = TwoStreamNetwork()\n",
        "\n",
        "# # Move model to device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# # Example input for a batch of 2 videos, each with 126 frames\n",
        "# rgb_input = torch.randn(2, 126, 3, 224, 224).to(device)  # RGB frames\n",
        "# flow_input = torch.randn(2, 126, 3, 224, 224).to(device)  # Flow frames\n",
        "\n",
        "# # Forward pass\n",
        "# outputs = model(rgb_input, flow_input)\n",
        "# for output in outputs:\n",
        "#     print(output.shape)  # Should print the shape of each output tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAhfnmDJ4U3_",
        "outputId": "5cc3b3c3-ef60-4473-ccff-c70077edd91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading annotations from: path/to/SoccerNet/mvfouls/train/annotations.json\n",
            "Total actions found in annotations: 2916\n",
            "\n",
            "Summary:\n",
            "Total actions loaded: 100\n",
            "Total actions skipped: 142\n",
            "Loading annotations from: path/to/SoccerNet/mvfouls/valid/annotations.json\n",
            "Total actions found in annotations: 411\n",
            "\n",
            "Summary:\n",
            "Total actions loaded: 64\n",
            "Total actions skipped: 36\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 7.8979 | Train Accuracies: {'action': 0.17, 'offence': 0.52, 'severity': 0.75, 'bodypart': 0.46, 'offence_severity': 0.52}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 16.4196 | Val Accuracies: {'action': 0.171875, 'offence': 0.203125, 'severity': 0.5625, 'bodypart': 0.421875, 'offence_severity': 0.21875}\n",
            "Saved best model.\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:14<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 6.6775 | Train Accuracies: {'action': 0.18, 'offence': 0.57, 'severity': 0.77, 'bodypart': 0.43, 'offence_severity': 0.61}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 7.4137 | Val Accuracies: {'action': 0.359375, 'offence': 0.359375, 'severity': 0.5625, 'bodypart': 0.421875, 'offence_severity': 0.625}\n",
            "Saved best model.\n",
            "\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 5.5753 | Train Accuracies: {'action': 0.2, 'offence': 0.63, 'severity': 0.76, 'bodypart': 0.53, 'offence_severity': 0.64}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 6.1830 | Val Accuracies: {'action': 0.28125, 'offence': 0.21875, 'severity': 0.5625, 'bodypart': 0.578125, 'offence_severity': 0.21875}\n",
            "Saved best model.\n",
            "\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 5.9257 | Train Accuracies: {'action': 0.19, 'offence': 0.63, 'severity': 0.73, 'bodypart': 0.48, 'offence_severity': 0.72}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 6.2357 | Val Accuracies: {'action': 0.234375, 'offence': 0.828125, 'severity': 0.5625, 'bodypart': 0.578125, 'offence_severity': 0.375}\n",
            "\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 5.6519 | Train Accuracies: {'action': 0.26, 'offence': 0.53, 'severity': 0.62, 'bodypart': 0.55, 'offence_severity': 0.61}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 8.3467 | Val Accuracies: {'action': 0.296875, 'offence': 0.203125, 'severity': 0.5625, 'bodypart': 0.40625, 'offence_severity': 0.21875}\n",
            "\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 5.8886 | Train Accuracies: {'action': 0.2, 'offence': 0.58, 'severity': 0.72, 'bodypart': 0.58, 'offence_severity': 0.58}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 6.9246 | Val Accuracies: {'action': 0.34375, 'offence': 0.265625, 'severity': 0.5625, 'bodypart': 0.5625, 'offence_severity': 0.796875}\n",
            "\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.8049 | Train Accuracies: {'action': 0.29, 'offence': 0.61, 'severity': 0.66, 'bodypart': 0.63, 'offence_severity': 0.72}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 6.1599 | Val Accuracies: {'action': 0.15625, 'offence': 0.46875, 'severity': 0.5625, 'bodypart': 0.546875, 'offence_severity': 0.5}\n",
            "Saved best model.\n",
            "\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.7196 | Train Accuracies: {'action': 0.3, 'offence': 0.74, 'severity': 0.78, 'bodypart': 0.57, 'offence_severity': 0.83}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 5.4313 | Val Accuracies: {'action': 0.171875, 'offence': 0.78125, 'severity': 0.5625, 'bodypart': 0.546875, 'offence_severity': 0.734375}\n",
            "Saved best model.\n",
            "\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.0278 | Train Accuracies: {'action': 0.45, 'offence': 0.65, 'severity': 0.79, 'bodypart': 0.59, 'offence_severity': 0.76}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 5.8051 | Val Accuracies: {'action': 0.375, 'offence': 0.609375, 'severity': 0.5625, 'bodypart': 0.515625, 'offence_severity': 0.703125}\n",
            "\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.3324 | Train Accuracies: {'action': 0.34, 'offence': 0.76, 'severity': 0.76, 'bodypart': 0.73, 'offence_severity': 0.71}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 8.1233 | Val Accuracies: {'action': 0.203125, 'offence': 0.296875, 'severity': 0.5625, 'bodypart': 0.359375, 'offence_severity': 0.234375}\n",
            "\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.2207 | Train Accuracies: {'action': 0.37, 'offence': 0.68, 'severity': 0.85, 'bodypart': 0.59, 'offence_severity': 0.75}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 7.0404 | Val Accuracies: {'action': 0.203125, 'offence': 0.6875, 'severity': 0.578125, 'bodypart': 0.421875, 'offence_severity': 0.4375}\n",
            "\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.0509 | Train Accuracies: {'action': 0.41, 'offence': 0.73, 'severity': 0.75, 'bodypart': 0.69, 'offence_severity': 0.69}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 5.8314 | Val Accuracies: {'action': 0.171875, 'offence': 0.71875, 'severity': 0.59375, 'bodypart': 0.453125, 'offence_severity': 0.65625}\n",
            "\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.5554 | Train Accuracies: {'action': 0.5, 'offence': 0.72, 'severity': 0.83, 'bodypart': 0.69, 'offence_severity': 0.83}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 5.1290 | Val Accuracies: {'action': 0.328125, 'offence': 0.796875, 'severity': 0.578125, 'bodypart': 0.390625, 'offence_severity': 0.84375}\n",
            "Saved best model.\n",
            "\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.7871 | Train Accuracies: {'action': 0.35, 'offence': 0.63, 'severity': 0.77, 'bodypart': 0.69, 'offence_severity': 0.63}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 10.9261 | Val Accuracies: {'action': 0.140625, 'offence': 0.21875, 'severity': 0.5625, 'bodypart': 0.421875, 'offence_severity': 0.21875}\n",
            "\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.4412 | Train Accuracies: {'action': 0.42, 'offence': 0.64, 'severity': 0.84, 'bodypart': 0.54, 'offence_severity': 0.74}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 5.5870 | Val Accuracies: {'action': 0.25, 'offence': 0.640625, 'severity': 0.59375, 'bodypart': 0.4375, 'offence_severity': 0.6875}\n",
            "\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.4598 | Train Accuracies: {'action': 0.42, 'offence': 0.78, 'severity': 0.85, 'bodypart': 0.65, 'offence_severity': 0.86}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 6.3781 | Val Accuracies: {'action': 0.125, 'offence': 0.34375, 'severity': 0.578125, 'bodypart': 0.578125, 'offence_severity': 0.625}\n",
            "\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.4218 | Train Accuracies: {'action': 0.55, 'offence': 0.68, 'severity': 0.88, 'bodypart': 0.68, 'offence_severity': 0.83}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 7.8406 | Val Accuracies: {'action': 0.296875, 'offence': 0.296875, 'severity': 0.5625, 'bodypart': 0.515625, 'offence_severity': 0.390625}\n",
            "\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.0153 | Train Accuracies: {'action': 0.45, 'offence': 0.67, 'severity': 0.85, 'bodypart': 0.66, 'offence_severity': 0.72}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 7.1777 | Val Accuracies: {'action': 0.15625, 'offence': 0.71875, 'severity': 0.578125, 'bodypart': 0.4375, 'offence_severity': 0.578125}\n",
            "\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.5689 | Train Accuracies: {'action': 0.49, 'offence': 0.78, 'severity': 0.87, 'bodypart': 0.66, 'offence_severity': 0.81}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 5.8281 | Val Accuracies: {'action': 0.328125, 'offence': 0.671875, 'severity': 0.578125, 'bodypart': 0.5625, 'offence_severity': 0.734375}\n",
            "\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.8561 | Train Accuracies: {'action': 0.51, 'offence': 0.75, 'severity': 0.8, 'bodypart': 0.64, 'offence_severity': 0.79}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 16/16 [00:08<00:00,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 7.0829 | Val Accuracies: {'action': 0.203125, 'offence': 0.375, 'severity': 0.609375, 'bodypart': 0.375, 'offence_severity': 0.640625}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "# Import your model\n",
        "#from model import TwoStreamNetwork  # Assuming the model code is saved as model.py\n",
        "\n",
        "# Custom Dataset class\n",
        "class ActionDataset(Dataset):\n",
        "    def __init__(self, rgb_clips, flow_clips, labels, transform=None):\n",
        "        self.rgb_clips = rgb_clips\n",
        "        self.flow_clips = flow_clips\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rgb_clips)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgb_frames = self.rgb_clips[idx]\n",
        "        flow_frames = self.flow_clips[idx]\n",
        "\n",
        "        # Apply transformation\n",
        "        if self.transform:\n",
        "            rgb_frames = [self.transform(frame) if not isinstance(frame, torch.Tensor) else frame for frame in rgb_frames]\n",
        "            flow_frames = [self.transform(frame) if not isinstance(frame, torch.Tensor) else frame for frame in flow_frames]\n",
        "\n",
        "        # Ensure dimensions are [num_frames, channels, height, width]\n",
        "        rgb_frames = torch.stack(rgb_frames, dim=0)\n",
        "        flow_frames = torch.stack(flow_frames, dim=0)\n",
        "\n",
        "        label_dict = {key: torch.tensor(self.labels[key][idx]) for key in self.labels.keys()}\n",
        "\n",
        "        return rgb_frames, flow_frames, label_dict\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = {key: [] for key in ['action', 'offence', 'severity', 'bodypart', 'offence_severity']}\n",
        "    all_labels = {key: [] for key in all_preds.keys()}\n",
        "\n",
        "    for rgb_input, flow_input, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        # Check input shapes and move to device\n",
        "        rgb_input, flow_input = rgb_input.to(device), flow_input.to(device)\n",
        "\n",
        "        # Verify dimensions; if missing batch dim, add it\n",
        "        if len(rgb_input.shape) == 4:\n",
        "            rgb_input = rgb_input.unsqueeze(0)  # Add batch dim if missing\n",
        "        if len(flow_input.shape) == 4:\n",
        "            flow_input = flow_input.unsqueeze(0)\n",
        "\n",
        "        labels = {key: val.to(device) for key, val in labels.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(rgb_input, flow_input)\n",
        "\n",
        "        # Compute losses for each task\n",
        "        loss = 0.0\n",
        "        for i, task in enumerate(all_preds.keys()):\n",
        "            task_loss = criterion(outputs[i], labels[task])\n",
        "            loss += task_loss\n",
        "            all_preds[task].extend(outputs[i].argmax(dim=1).cpu().numpy())\n",
        "            all_labels[task].extend(labels[task].cpu().numpy())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = {task: accuracy_score(all_labels[task], all_preds[task]) for task in all_preds.keys()}\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Validation function\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = {key: [] for key in ['action', 'offence', 'severity', 'bodypart', 'offence_severity']}\n",
        "    all_labels = {key: [] for key in all_preds.keys()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for rgb_input, flow_input, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            rgb_input, flow_input = rgb_input.to(device), flow_input.to(device)\n",
        "            labels = {key: val.to(device) for key, val in labels.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(rgb_input, flow_input)\n",
        "\n",
        "            # Compute losses and predictions for each task\n",
        "            loss = 0.0\n",
        "            for i, task in enumerate(all_preds.keys()):\n",
        "                task_loss = criterion(outputs[i], labels[task])\n",
        "                loss += task_loss\n",
        "                all_preds[task].extend(outputs[i].argmax(dim=1).cpu().numpy())\n",
        "                all_labels[task].extend(labels[task].cpu().numpy())\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = {task: accuracy_score(all_labels[task], all_preds[task]) for task in all_preds.keys()}\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def main(data_path, num_epochs=20, batch_size=4, learning_rate=1e-3, max_samples=50):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load data\n",
        "    train_rgb_clips, train_flow_clips, train_labels_action, train_labels_offence, train_labels_severity, train_labels_bodypart, train_labels_offence_severity = \\\n",
        "        load_filtered_clips_and_labels(data_path, \"train\", max_samples)\n",
        "\n",
        "    valid_rgb_clips, valid_flow_clips, valid_labels_action, valid_labels_offence, valid_labels_severity, valid_labels_bodypart, valid_labels_offence_severity = \\\n",
        "        load_filtered_clips_and_labels(data_path, \"valid\", max_samples)\n",
        "\n",
        "    # Organize labels in a dictionary format\n",
        "    train_labels = {\n",
        "        \"action\": train_labels_action,\n",
        "        \"offence\": train_labels_offence,\n",
        "        \"severity\": train_labels_severity,\n",
        "        \"bodypart\": train_labels_bodypart,\n",
        "        \"offence_severity\": train_labels_offence_severity\n",
        "    }\n",
        "    valid_labels = {\n",
        "        \"action\": valid_labels_action,\n",
        "        \"offence\": valid_labels_offence,\n",
        "        \"severity\": valid_labels_severity,\n",
        "        \"bodypart\": valid_labels_bodypart,\n",
        "        \"offence_severity\": valid_labels_offence_severity\n",
        "    }\n",
        "\n",
        "    # Define transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((112, 112)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_dataset = ActionDataset(train_rgb_clips, train_flow_clips, train_labels, transform=transform)\n",
        "    valid_dataset = ActionDataset(valid_rgb_clips, valid_flow_clips, valid_labels, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    model = TwoStreamNetwork().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training and validation loop\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Accuracies: {train_accuracy}\")\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_accuracy = validate(model, valid_loader, criterion, device)\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Accuracies: {val_accuracy}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(\"Saved best model.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Update this path with your actual data path\n",
        "    DATA_PATH = 'path/to/SoccerNet/mvfouls'\n",
        "    main(data_path=DATA_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLJ0mKdX4U4A",
        "outputId": "d3c5569f-0b81-4759-c83b-c4180f5ae868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading annotations from: path/to/SoccerNet/mvfouls/test/annotations.json\n",
            "Total actions found in annotations: 301\n",
            "\n",
            "Summary:\n",
            "Total actions loaded: 20\n",
            "Total actions skipped: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c6d5566736a9>:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n",
            "Testing: 100%|██████████| 5/5 [00:02<00:00,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 5.2123 | Test Accuracies: {'action': 0.3, 'offence': 0.55, 'severity': 0.75, 'bodypart': 0.4, 'offence_severity': 0.55}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Test function\n",
        "def test(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = {key: [] for key in ['action', 'offence', 'severity', 'bodypart', 'offence_severity']}\n",
        "    all_labels = {key: [] for key in all_preds.keys()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for rgb_input, flow_input, labels in tqdm(dataloader, desc=\"Testing\"):\n",
        "            rgb_input, flow_input = rgb_input.to(device), flow_input.to(device)\n",
        "            labels = {key: val.to(device) for key, val in labels.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(rgb_input, flow_input)\n",
        "\n",
        "            # Compute losses and predictions for each task\n",
        "            loss = 0.0\n",
        "            for i, task in enumerate(all_preds.keys()):\n",
        "                task_loss = criterion(outputs[i], labels[task])\n",
        "                loss += task_loss\n",
        "                all_preds[task].extend(outputs[i].argmax(dim=1).cpu().numpy())\n",
        "                all_labels[task].extend(labels[task].cpu().numpy())\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    accuracy = {task: accuracy_score(all_labels[task], all_preds[task]) for task in all_preds.keys()}\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Main testing function\n",
        "def main_test(data_path, batch_size=4, max_samples=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load test data\n",
        "    test_rgb_clips, test_flow_clips, test_labels_action, test_labels_offence, test_labels_severity, test_labels_bodypart, test_labels_offence_severity = load_filtered_clips_and_labels(data_path, \"test\", max_samples)\n",
        "\n",
        "    # Organize labels in a dictionary format\n",
        "    test_labels = {\n",
        "        \"action\": test_labels_action,\n",
        "        \"offence\": test_labels_offence,\n",
        "        \"severity\": test_labels_severity,\n",
        "        \"bodypart\": test_labels_bodypart,\n",
        "        \"offence_severity\": test_labels_offence_severity\n",
        "    }\n",
        "\n",
        "    # Define transform (same as used in training)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((56, 56)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create test dataset and loader\n",
        "    test_dataset = ActionDataset(test_rgb_clips, test_flow_clips, test_labels, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model and load best weights\n",
        "    model = TwoStreamNetwork().to(device)\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "\n",
        "    # Define the loss criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Test the model\n",
        "    test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracies: {test_accuracy}\")\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    # Update this path with your actual data path\n",
        "    DATA_PATH = 'path/to/SoccerNet/mvfouls'\n",
        "    main_test(data_path=DATA_PATH)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}