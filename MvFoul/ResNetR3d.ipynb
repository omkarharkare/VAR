{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klvJIrzJP5FI",
        "outputId": "feddcc67-5fca-4930-df9f-259ccf8d420e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SoccerNet\n",
            "  Downloading SoccerNet-0.1.61-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (4.66.6)\n",
            "Collecting scikit-video (from SoccerNet)\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (3.8.0)\n",
            "Collecting google-measurement-protocol (from SoccerNet)\n",
            "  Downloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl.metadata (845 bytes)\n",
            "Collecting pycocoevalcap (from SoccerNet)\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: huggingface-hub[cli] in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (0.24.7)\n",
            "Requirement already satisfied: requests<3.0a0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from google-measurement-protocol->SoccerNet) (2.32.3)\n",
            "Collecting prices>=1.0.0 (from google-measurement-protocol->SoccerNet)\n",
            "  Downloading prices-1.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (4.12.2)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli]->SoccerNet)\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet)\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (3.0.48)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.26.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (2.8.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap->SoccerNet) (2.0.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video->SoccerNet) (1.13.1)\n",
            "Requirement already satisfied: babel>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from prices>=1.0.0->google-measurement-protocol->SoccerNet) (2.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->SoccerNet) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2024.8.30)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (0.2.13)\n",
            "Downloading SoccerNet-0.1.61-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prices-1.1.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: prices, pfzy, scikit-video, InquirerPy, google-measurement-protocol, pycocoevalcap, SoccerNet\n",
            "Successfully installed InquirerPy-0.3.4 SoccerNet-0.1.61 google-measurement-protocol-1.1.0 pfzy-0.3.4 prices-1.1.1 pycocoevalcap-1.2 scikit-video-1.1.11\n"
          ]
        }
      ],
      "source": [
        "%pip install SoccerNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDCl0PvRQCfy",
        "outputId": "1a324600-df3e-4fb1-ebb4-f1aafbf7e821"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading path/to/SoccerNet/mvfouls/train.zip...: : 2.46GiB [02:23, 17.1MiB/s]                         \n",
            "Downloading path/to/SoccerNet/mvfouls/valid.zip...: : 351MiB [00:22, 15.7MiB/s]                        \n",
            "Downloading path/to/SoccerNet/mvfouls/test.zip...: : 268MiB [00:18, 14.6MiB/s]                        \n",
            "Downloading path/to/SoccerNet/mvfouls/challenge.zip...: : 246MiB [00:16, 14.7MiB/s]                        \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted train.zip to path/to/SoccerNet/mvfouls/train\n",
            "Extracted valid.zip to path/to/SoccerNet/mvfouls/valid\n",
            "Extracted test.zip to path/to/SoccerNet/mvfouls/test\n",
            "Extracted challenge.zip to path/to/SoccerNet/mvfouls/challenge\n",
            "Removed train.zip\n",
            "Removed valid.zip\n",
            "Removed test.zip\n",
            "Removed challenge.zip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from SoccerNet.Downloader import SoccerNetDownloader as SNdl\n",
        "\n",
        "# Set up the downloader\n",
        "local_directory = \"path/to/SoccerNet\"\n",
        "mySNdl = SNdl(LocalDirectory=local_directory)\n",
        "\n",
        "# Download the data\n",
        "mySNdl.downloadDataTask(task=\"mvfouls\", split=[\"train\", \"valid\", \"test\", \"challenge\"], password=\"s0cc3rn3t\")\n",
        "\n",
        "# Unzip the downloaded files\n",
        "task_directory = os.path.join(local_directory, \"mvfouls\")\n",
        "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
        "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
        "    if os.path.exists(zip_file):\n",
        "        # Create a new folder with the same name as the zip file\n",
        "        extract_folder = os.path.join(task_directory, split)\n",
        "        os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "        # Extract the contents to the new folder\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_folder)\n",
        "        print(f\"Extracted {split}.zip to {extract_folder}\")\n",
        "    else:\n",
        "        print(f\"{split}.zip not found\")\n",
        "\n",
        "# Optionally, remove the zip files after extraction\n",
        "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
        "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
        "    if os.path.exists(zip_file):\n",
        "        os.remove(zip_file)\n",
        "        print(f\"Removed {split}.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W3gXplzpQH2L"
      },
      "outputs": [],
      "source": [
        "# Class name to label index\n",
        "\n",
        "EVENT_DICTIONARY_action_class = {\"Tackling\":0,\"Standing tackling\":1,\"High leg\":2,\"Holding\":3,\"Pushing\":4,\n",
        "                        \"Elbowing\":5, \"Challenge\":6, \"Dive\":7, \"Dont know\":8}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_action_class = {0:\"Tackling\", 1:\"Standing tackling\", 2:\"High leg\", 3:\"Holding\", 4:\"Pushing\",\n",
        "                        5:\"Elbowing\", 6:\"Challenge\", 7:\"Dive\", 8:\"Dont know\"}\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY_offence_severity_class = {\"No offence\":0,\"Offence + No card\":1,\"Offence + Yellow card\":2,\"Offence + Red card\":3}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_offence_severity_class = {0:\"No offence\", 1:\"Offence + No card\", 2:\"Offence + Yellow card\", 3:\"Offence + Red card\"}\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY_offence_class = {\"Offence\":0,\"Between\":1,\"No Offence\":2, \"No offence\":2}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_offence_class = {0:\"Offence\", 1:\"Between\", 2:\"No Offence\"}\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY_severity_class = {\"1.0\":0,\"2.0\":1,\"3.0\":2,\"4.0\":3,\"5.0\":4}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_severity_class = {0:\"No card\", 1:\"Borderline No/Yellow\", 2:\"Yellow card\", 3:\"Borderline Yellow/Red\", 4:\"Red card\"}\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY_bodypart_class = {\"Upper body\":0,\"Under body\":1}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_bodypart_class = {0:\"Upper body\", 1:\"Under body\"}\n",
        "\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY = {'action_class':EVENT_DICTIONARY_action_class, 'offence_class': EVENT_DICTIONARY_offence_class,\n",
        "            'severity_class': EVENT_DICTIONARY_severity_class, 'bodypart_class': EVENT_DICTIONARY_bodypart_class, 'offence_severity_class': EVENT_DICTIONARY_offence_severity_class}\n",
        "INVERSE_EVENT_DICTIONARY = {'action_class':INVERSE_EVENT_DICTIONARY_action_class, 'offence_class': INVERSE_EVENT_DICTIONARY_offence_class,\n",
        "            'severity_class': INVERSE_EVENT_DICTIONARY_severity_class, 'bodypart_class': INVERSE_EVENT_DICTIONARY_bodypart_class, 'offence_severity_class': INVERSE_EVENT_DICTIONARY_offence_severity_class}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pUfIzHn2QLjY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#DATA_PATH = 'path/to/SoccerNet/mvfouls'\n",
        "\n",
        "# Set the desired frame count\n",
        "DESIRED_FRAME_COUNT = 126\n",
        "\n",
        "# Load the EVENT_DICTIONARY for mapping annotation labels\n",
        "EVENT_DICTIONARY = {\n",
        "    'action_class': {\"Tackling\": 0, \"Standing tackling\": 1, \"High leg\": 2, \"Holding\": 3, \"Pushing\": 4,\n",
        "                     \"Elbowing\": 5, \"Challenge\": 6, \"Dive\": 7, \"Dont know\": 8},\n",
        "    'offence_class': {\"Offence\": 0, \"Between\": 1, \"No Offence\": 2, \"No offence\": 2},\n",
        "    'severity_class': {\"1.0\": 0, \"2.0\": 1, \"3.0\": 2, \"4.0\": 3, \"5.0\": 4},\n",
        "    'bodypart_class': {\"Upper body\": 0, \"Under body\": 1},\n",
        "    'offence_severity_class': {\"No offence\": 0, \"Offence + No card\": 1, \"Offence + Yellow card\": 2, \"Offence + Red card\": 3}\n",
        "}\n",
        "\n",
        "# Transformation for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_filtered_clips_and_labels(DATA_PATH,split, max_samples):\n",
        "    clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = [], [], [], [], [], []\n",
        "\n",
        "    annotations_path = os.path.join(DATA_PATH, split, \"annotations.json\")\n",
        "    print(f\"Loading annotations from: {annotations_path}\")\n",
        "\n",
        "    with open(annotations_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "    print(f\"Total actions found in annotations: {len(annotations['Actions'])}\")\n",
        "\n",
        "    offence_count, no_offence_count, skipped_actions = 0, 0, 0\n",
        "    max_samples = max_samples  # Maximum samples for each class\n",
        "\n",
        "    for action_index, (action_key, action_data) in enumerate(annotations['Actions'].items()):\n",
        "        offence_class = action_data['Offence']\n",
        "\n",
        "        # Filter only 50 samples for each class\n",
        "        if (offence_class == \"Offence\" and offence_count >= max_samples) or \\\n",
        "           (offence_class in [\"No offence\", \"No Offence\"] and no_offence_count >= max_samples):\n",
        "            continue\n",
        "\n",
        "        action_class = action_data['Action class']\n",
        "        severity_class = action_data['Severity'] if action_data['Severity'].replace('.', '').isdigit() else '1.0'\n",
        "        bodypart_class = action_data.get('Bodypart', 'Upper body')\n",
        "        offence_severity = f\"{offence_class} + {EVENT_DICTIONARY['severity_class'].get(severity_class, 'No card')}\"\n",
        "\n",
        "        action_label = EVENT_DICTIONARY['action_class'].get(action_class)\n",
        "        offence_label = EVENT_DICTIONARY['offence_class'].get(offence_class)\n",
        "        severity_label = EVENT_DICTIONARY['severity_class'].get(severity_class)\n",
        "        bodypart_label = EVENT_DICTIONARY['bodypart_class'].get(bodypart_class)\n",
        "        offence_severity_label = EVENT_DICTIONARY['offence_severity_class'].get(offence_severity, 0)\n",
        "\n",
        "        if None in [action_label, offence_label, severity_label, bodypart_label, offence_severity_label]:\n",
        "            skipped_actions += 1\n",
        "            continue\n",
        "\n",
        "        action_folder = os.path.join(DATA_PATH, split, f\"action_{action_key}\")\n",
        "\n",
        "        if not os.path.exists(action_folder):\n",
        "            skipped_actions += 1\n",
        "            continue\n",
        "\n",
        "        action_clips = []\n",
        "        for clip_idx in range(2):\n",
        "            clip_path = os.path.join(action_folder, f\"clip_{clip_idx}.mp4\")\n",
        "            if not os.path.exists(clip_path):\n",
        "                continue\n",
        "\n",
        "            cap = cv2.VideoCapture(clip_path)\n",
        "            frames = []\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = Image.fromarray(frame)\n",
        "                frame = transform(frame)\n",
        "                frames.append(frame)\n",
        "            cap.release()\n",
        "\n",
        "            # Sample or pad frames to ensure uniform frame count\n",
        "            if len(frames) > DESIRED_FRAME_COUNT:\n",
        "                indices = np.linspace(0, len(frames) - 1, DESIRED_FRAME_COUNT).astype(int)\n",
        "                frames = [frames[i] for i in indices]\n",
        "            elif len(frames) < DESIRED_FRAME_COUNT:\n",
        "                frames += [frames[-1]] * (DESIRED_FRAME_COUNT - len(frames))\n",
        "\n",
        "            video_tensor = torch.stack(frames, dim=0)\n",
        "            action_clips.append(video_tensor)\n",
        "\n",
        "            # Print tensor shape and size for debugging\n",
        "            print(f\"Action {action_key}, Clip {clip_idx} tensor shape: {video_tensor.shape}\")\n",
        "            tensor_size = video_tensor.element_size() * video_tensor.nelement() / (1024**2)\n",
        "            print(f\"Tensor size: {tensor_size:.2f} MB\")\n",
        "\n",
        "        if action_clips:\n",
        "            clips.append(action_clips)\n",
        "            labels_action.append(action_label)\n",
        "            labels_offence.append(offence_label)\n",
        "            labels_severity.append(severity_label)\n",
        "            labels_bodypart.append(bodypart_label)\n",
        "            labels_offence_severity.append(offence_severity_label)\n",
        "\n",
        "            if offence_class == \"Offence\":\n",
        "                offence_count += 1\n",
        "            else:\n",
        "                no_offence_count += 1\n",
        "\n",
        "            print(f\"Added action {action_key} with {len(action_clips)} clips to dataset.\")\n",
        "\n",
        "        # Stop if we have 50 samples for each class\n",
        "        if offence_count >= max_samples and no_offence_count >= max_samples:\n",
        "            break\n",
        "\n",
        "    print(\"\\nSummary:\")\n",
        "    print(f\"Total actions loaded: {len(clips)}\")\n",
        "    print(f\"Total actions skipped: {skipped_actions}\")\n",
        "    if clips:\n",
        "        print(f\"Example clip shape: {clips[0][0].shape}\")\n",
        "        print(f\"First action label: {labels_action[0]}\")\n",
        "\n",
        "    return clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity\n",
        "\n",
        "\n",
        "# # Load datasets for each split\n",
        "# datasets = {}\n",
        "# for split in ['train', 'valid', 'test']:\n",
        "#     clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = load_filtered_clips_and_labels(DATA_PATH,split)\n",
        "#     datasets[split] = {\n",
        "#         'clips': clips,\n",
        "#         'labels': {\n",
        "#             'action': labels_action,\n",
        "#             'offence': labels_offence,\n",
        "#             'severity': labels_severity,\n",
        "#             'bodypart': labels_bodypart,\n",
        "#             'offence_severity': labels_offence_severity\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "# # Display dataset info for verification\n",
        "# for split in ['train', 'valid', 'test']:\n",
        "#     print(f\"{split.capitalize()} set:\")\n",
        "#     print(\"Number of samples:\", len(datasets[split]['clips']))\n",
        "#     print(\"Example label structure:\", datasets[split]['labels']['action'][0] if datasets[split]['labels']['action'] else \"No labels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "k7dkF2IBQQi0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MultiTaskActionRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes_action=9, num_classes_offence=3, num_classes_severity=5,\n",
        "                 num_classes_bodypart=2, num_classes_offence_severity=4, freeze_backbone=True):\n",
        "        super(MultiTaskActionRecognitionModel, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet-18 3D backbone\n",
        "        r3d_18 = models.video.r3d_18(pretrained=True)\n",
        "\n",
        "        # Optionally freeze backbone layers\n",
        "        if freeze_backbone:\n",
        "            for param in r3d_18.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Remove the last fully connected layer\n",
        "        self.backbone = nn.Sequential(*list(r3d_18.children())[:-1])  # Output shape will be [batch, 512, 1, 1, 1]\n",
        "\n",
        "        # Define custom fully connected layers for each task\n",
        "        self.fc_action = nn.Linear(512, num_classes_action)\n",
        "        self.fc_offence = nn.Linear(512, num_classes_offence)\n",
        "        self.fc_severity = nn.Linear(512, num_classes_severity)\n",
        "        self.fc_bodypart = nn.Linear(512, num_classes_bodypart)\n",
        "        self.fc_offence_severity = nn.Linear(512, num_classes_offence_severity)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features using the backbone\n",
        "        x = self.backbone(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to [batch, 512]\n",
        "\n",
        "        # Forward through task-specific fully connected layers\n",
        "        action_out = self.fc_action(x)\n",
        "        offence_out = self.fc_offence(x)\n",
        "        severity_out = self.fc_severity(x)\n",
        "        bodypart_out = self.fc_bodypart(x)\n",
        "        offence_severity_out = self.fc_offence_severity(x)\n",
        "\n",
        "        return action_out, offence_out, severity_out, bodypart_out, offence_severity_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbyJvaEhQULj",
        "outputId": "4542d131-7efd-4f79-f74b-be2634d138d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading annotations from: mvfouls\\train\\annotations.json\n",
            "Total actions found in annotations: 2916\n",
            "Action 0, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 0, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 0 with 2 clips to dataset.\n",
            "Action 1, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 1, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 1 with 2 clips to dataset.\n",
            "Action 2, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 2, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 2 with 2 clips to dataset.\n",
            "Action 3, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 3, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 3 with 2 clips to dataset.\n",
            "Action 4, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 4, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 4 with 2 clips to dataset.\n",
            "Action 5, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 5, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 5 with 2 clips to dataset.\n",
            "Action 6, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 6, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 6 with 2 clips to dataset.\n",
            "Action 7, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 7, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 7 with 2 clips to dataset.\n",
            "Action 9, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 9, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 9 with 2 clips to dataset.\n",
            "Action 10, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 10, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 10 with 2 clips to dataset.\n",
            "Action 11, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 11, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 11 with 2 clips to dataset.\n",
            "Action 12, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 12, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 12 with 2 clips to dataset.\n",
            "Action 13, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 13, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 13 with 2 clips to dataset.\n",
            "Action 14, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 14, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 14 with 2 clips to dataset.\n",
            "Action 15, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 15, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 15 with 2 clips to dataset.\n",
            "Action 19, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 19, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 19 with 2 clips to dataset.\n",
            "Action 25, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 25, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 25 with 2 clips to dataset.\n",
            "Action 32, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 32, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 32 with 2 clips to dataset.\n",
            "Action 33, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 33, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 33 with 2 clips to dataset.\n",
            "Action 38, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 38, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 38 with 2 clips to dataset.\n",
            "Action 40, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 40, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 40 with 2 clips to dataset.\n",
            "Action 55, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 55, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 55 with 2 clips to dataset.\n",
            "Action 58, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 58, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 58 with 2 clips to dataset.\n",
            "Action 63, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 63, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 63 with 2 clips to dataset.\n",
            "Action 85, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 85, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 85 with 2 clips to dataset.\n",
            "Action 98, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 98, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 98 with 2 clips to dataset.\n",
            "Action 121, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 121, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 121 with 2 clips to dataset.\n",
            "Action 132, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 132, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 132 with 2 clips to dataset.\n",
            "Action 151, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 151, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 151 with 2 clips to dataset.\n",
            "Action 162, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 162, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 162 with 2 clips to dataset.\n",
            "\n",
            "Summary:\n",
            "Total actions loaded: 30\n",
            "Total actions skipped: 1\n",
            "Example clip shape: torch.Size([126, 3, 32, 32])\n",
            "First action label: 6\n",
            "Loading annotations from: mvfouls\\valid\\annotations.json\n",
            "Total actions found in annotations: 411\n",
            "Action 0, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 0, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 0 with 2 clips to dataset.\n",
            "Action 1, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 1, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 1 with 2 clips to dataset.\n",
            "Action 2, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 2, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 2 with 2 clips to dataset.\n",
            "Action 3, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 3, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 3 with 2 clips to dataset.\n",
            "Action 4, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 4, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 4 with 2 clips to dataset.\n",
            "Action 5, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 5, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 5 with 2 clips to dataset.\n",
            "Action 6, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 6, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 6 with 2 clips to dataset.\n",
            "Action 7, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 7, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 7 with 2 clips to dataset.\n",
            "Action 8, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 8, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 8 with 2 clips to dataset.\n",
            "Action 9, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 9, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 9 with 2 clips to dataset.\n",
            "Action 10, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 10, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 10 with 2 clips to dataset.\n",
            "Action 11, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 11, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 11 with 2 clips to dataset.\n",
            "Action 12, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 12, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 12 with 2 clips to dataset.\n",
            "Action 13, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 13, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 13 with 2 clips to dataset.\n",
            "Action 14, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 14, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 14 with 2 clips to dataset.\n",
            "Action 15, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 15, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 15 with 2 clips to dataset.\n",
            "Action 16, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 16, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 16 with 2 clips to dataset.\n",
            "Action 17, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 17, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 17 with 2 clips to dataset.\n",
            "Action 20, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 20, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 20 with 2 clips to dataset.\n",
            "Action 22, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 22, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 22 with 2 clips to dataset.\n",
            "Action 39, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 39, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 39 with 2 clips to dataset.\n",
            "Action 50, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 50, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 50 with 2 clips to dataset.\n",
            "Action 64, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 64, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 64 with 2 clips to dataset.\n",
            "Action 74, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 74, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 74 with 2 clips to dataset.\n",
            "Action 83, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 83, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 83 with 2 clips to dataset.\n",
            "Action 85, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 85, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 85 with 2 clips to dataset.\n",
            "Action 86, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 86, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 86 with 2 clips to dataset.\n",
            "Action 88, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 88, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 88 with 2 clips to dataset.\n",
            "Action 96, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 96, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 96 with 2 clips to dataset.\n",
            "Action 108, Clip 0 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Action 108, Clip 1 tensor shape: torch.Size([126, 3, 32, 32])\n",
            "Tensor size: 1.48 MB\n",
            "Added action 108 with 2 clips to dataset.\n",
            "\n",
            "Summary:\n",
            "Total actions loaded: 30\n",
            "Total actions skipped: 1\n",
            "Example clip shape: torch.Size([126, 3, 32, 32])\n",
            "First action label: 3\n",
            "Data loaded\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.70 GiB is allocated by PyTorch, and 28.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 109\u001b[0m\n\u001b[0;32m    106\u001b[0m offence_severity_labels \u001b[38;5;241m=\u001b[39m offence_severity_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Forward pass with mixed precision\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m action_out, offence_out, severity_out, bodypart_out, offence_severity_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m loss \u001b[38;5;241m=\u001b[39m (criterion(action_out, action_labels) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    111\u001b[0m         criterion(offence_out, offence_labels) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    112\u001b[0m         criterion(severity_out, severity_labels) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    113\u001b[0m         criterion(bodypart_out, bodypart_labels) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    114\u001b[0m         criterion(offence_severity_out, offence_severity_labels))\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Update gradients only after accumulation steps\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[19], line 30\u001b[0m, in \u001b[0;36mMultiTaskActionRecognitionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Extract features using the backbone\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten to [batch, 512]\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Forward through task-specific fully connected layers\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    710\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    719\u001b[0m     )\n\u001b[1;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.70 GiB is allocated by PyTorch, and 28.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "#from model import MultiTaskActionRecognitionModel\n",
        "#from data import load_filtered_clips_and_labels\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define parameters\n",
        "batch_size = 2\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "max_samples = 15\n",
        "fixed_frame_count = 126  # Standardized frame count for all clips\n",
        "DATA_PATH = 'mvfouls'\n",
        "\n",
        "# Transformation for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class ActionDataset(Dataset):\n",
        "    def __init__(self, clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity):\n",
        "        self.clips = clips\n",
        "        self.labels_action = labels_action\n",
        "        self.labels_offence = labels_offence\n",
        "        self.labels_severity = labels_severity\n",
        "        self.labels_bodypart = labels_bodypart\n",
        "        self.labels_offence_severity = labels_offence_severity\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clips)\n",
        "\n",
        "    def pad_or_truncate(self, clip):\n",
        "        # Pad or truncate each clip to fixed frame count\n",
        "        frames = []\n",
        "        for i, frame in enumerate(clip):\n",
        "            if not isinstance(frame, torch.Tensor):  # Ensure the frame is not already a tensor\n",
        "                frame = transform(frame)  # Apply transforms\n",
        "            frames.append(frame)\n",
        "            if len(frames) == fixed_frame_count:  # Truncate\n",
        "                break\n",
        "        # Pad if necessary\n",
        "        while len(frames) < fixed_frame_count:\n",
        "            frames.append(torch.zeros_like(frames[0]))  # Padding with empty frames\n",
        "        return torch.stack(frames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip = self.clips[idx]\n",
        "        padded_clip = self.pad_or_truncate(clip)\n",
        "        return (padded_clip,\n",
        "                self.labels_action[idx],\n",
        "                self.labels_offence[idx],\n",
        "                self.labels_severity[idx],\n",
        "                self.labels_bodypart[idx],\n",
        "                self.labels_offence_severity[idx])\n",
        "\n",
        "# Load filtered dataset\n",
        "train_clips, train_labels_action, train_labels_offence, train_labels_severity, train_labels_bodypart, train_labels_offence_severity = load_filtered_clips_and_labels(DATA_PATH, \"train\", max_samples=max_samples)\n",
        "valid_clips, valid_labels_action, valid_labels_offence, valid_labels_severity, valid_labels_bodypart, valid_labels_offence_severity = load_filtered_clips_and_labels(DATA_PATH,\"valid\", max_samples=max_samples)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = ActionDataset(train_clips, train_labels_action, train_labels_offence, train_labels_severity, train_labels_bodypart, train_labels_offence_severity)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset = ActionDataset(valid_clips, valid_labels_action, valid_labels_offence, valid_labels_severity, valid_labels_bodypart, valid_labels_offence_severity)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "print('Data loaded')\n",
        "\n",
        "# Reshape the inputs to [batch_size, frames_clip_0 * frames_clip_1, channels, height, width]\n",
        "def reshape_tensor(x):\n",
        "    b, f0, f1, c, h, w = x.shape\n",
        "    return x.view(b, f0*f1, c, h, w).permute(0, 2, 1, 3, 4)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = MultiTaskActionRecognitionModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "accumulation_steps = 4  # Number of steps to accumulate gradients\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    optimizer.zero_grad()  # Reset gradients at the beginning of each epoch\n",
        "\n",
        "    for i, (inputs, action_labels, offence_labels, severity_labels, bodypart_labels, offence_severity_labels) in enumerate(train_loader):\n",
        "        inputs = reshape_tensor(inputs).to(device)\n",
        "\n",
        "        # Move labels to device\n",
        "        action_labels = action_labels.to(device)\n",
        "        offence_labels = offence_labels.to(device)\n",
        "        severity_labels = severity_labels.to(device)\n",
        "        bodypart_labels = bodypart_labels.to(device)\n",
        "        offence_severity_labels = offence_severity_labels.to(device)\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        action_out, offence_out, severity_out, bodypart_out, offence_severity_out = model(inputs)\n",
        "        loss = (criterion(action_out, action_labels) +\n",
        "                criterion(offence_out, offence_labels) +\n",
        "                criterion(severity_out, severity_labels) +\n",
        "                criterion(bodypart_out, bodypart_labels) +\n",
        "                criterion(offence_severity_out, offence_severity_labels))\n",
        "\n",
        "\n",
        "        # Update gradients only after accumulation steps\n",
        "        optimizer.zero_grad()  # Reset gradients for the next accumulation\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad():  # Disable gradient tracking for validation\n",
        "        for inputs, action_labels, offence_labels, severity_labels, bodypart_labels, offence_severity_labels in valid_loader:\n",
        "            # Check the shape of inputs\n",
        "            #[batch_size, frames_clip_0, frames_clip_1, channels, height, width]\n",
        "            print(f\"Input shape before adjustment: {inputs.shape}\")  # Should print [2, 126, 126, 3, 64, 64] per action\n",
        "\n",
        "            inputs = reshape_tensor(inputs)\n",
        "\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            action_labels = action_labels.to(device)\n",
        "            offence_labels = offence_labels.to(device)\n",
        "            severity_labels = severity_labels.to(device)\n",
        "            bodypart_labels = bodypart_labels.to(device)\n",
        "            offence_severity_labels = offence_severity_labels.to(device)\n",
        "\n",
        "            action_out, offence_out, severity_out, bodypart_out, offence_severity_out = model(inputs)\n",
        "\n",
        "            # Calculate validation loss\n",
        "            loss_action = criterion(action_out, action_labels)\n",
        "            loss_offence = criterion(offence_out, offence_labels)\n",
        "            loss_severity = criterion(severity_out, severity_labels)\n",
        "            loss_bodypart = criterion(bodypart_out, bodypart_labels)\n",
        "            loss_offence_severity = criterion(offence_severity_out, offence_severity_labels)\n",
        "\n",
        "            loss = loss_action + loss_offence + loss_severity + loss_bodypart + loss_offence_severity\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    print(f\"Validation Loss: {valid_loss / len(valid_loader):.4f}\")\n",
        "    \n",
        "    # After each training epoch\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save the model\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"saved_models/action_recognition_model.pth\")\n",
        "print(\"Model saved!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
