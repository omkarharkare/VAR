{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQI8DJC8eFhF",
        "outputId": "b79607a9-aaa0-4e01-a8b9-7fef4b3286a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SoccerNet\n",
            "  Downloading SoccerNet-0.1.61-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (4.66.6)\n",
            "Collecting scikit-video (from SoccerNet)\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (3.8.0)\n",
            "Collecting google-measurement-protocol (from SoccerNet)\n",
            "  Downloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl.metadata (845 bytes)\n",
            "Collecting pycocoevalcap (from SoccerNet)\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: huggingface-hub[cli] in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (0.24.7)\n",
            "Requirement already satisfied: requests<3.0a0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from google-measurement-protocol->SoccerNet) (2.32.3)\n",
            "Collecting prices>=1.0.0 (from google-measurement-protocol->SoccerNet)\n",
            "  Downloading prices-1.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (4.12.2)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli]->SoccerNet)\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet)\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (3.0.48)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.26.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (2.8.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap->SoccerNet) (2.0.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video->SoccerNet) (1.13.1)\n",
            "Requirement already satisfied: babel>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from prices>=1.0.0->google-measurement-protocol->SoccerNet) (2.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->SoccerNet) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2024.8.30)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (0.2.13)\n",
            "Downloading SoccerNet-0.1.61-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prices-1.1.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: prices, pfzy, scikit-video, InquirerPy, google-measurement-protocol, pycocoevalcap, SoccerNet\n",
            "Successfully installed InquirerPy-0.3.4 SoccerNet-0.1.61 google-measurement-protocol-1.1.0 pfzy-0.3.4 prices-1.1.1 pycocoevalcap-1.2 scikit-video-1.1.11\n"
          ]
        }
      ],
      "source": [
        "%pip install SoccerNet\n",
        "%pip install pytorchvideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsCegawieHO0",
        "outputId": "5fd7f5b7-fa0c-424a-bea5-9505f47dd64f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading path/to/SoccerNet/mvfouls/train.zip...:  14%|█▎        | 333M/2.46G [00:07<00:38, 54.7MiB/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from SoccerNet.Downloader import SoccerNetDownloader as SNdl\n",
        "\n",
        "# Set up the downloader\n",
        "local_directory = \"path/to/SoccerNet\"\n",
        "mySNdl = SNdl(LocalDirectory=local_directory)\n",
        "\n",
        "# Download the data\n",
        "mySNdl.downloadDataTask(task=\"mvfouls\", split=[\"train\", \"valid\", \"test\", \"challenge\"], password=\"s0cc3rn3t\")\n",
        "\n",
        "# Unzip the downloaded files\n",
        "task_directory = os.path.join(local_directory, \"mvfouls\")\n",
        "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
        "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
        "    if os.path.exists(zip_file):\n",
        "        # Create a new folder with the same name as the zip file\n",
        "        extract_folder = os.path.join(task_directory, split)\n",
        "        os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "        # Extract the contents to the new folder\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_folder)\n",
        "        print(f\"Extracted {split}.zip to {extract_folder}\")\n",
        "    else:\n",
        "        print(f\"{split}.zip not found\")\n",
        "\n",
        "# Optionally, remove the zip files after extraction\n",
        "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
        "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
        "    if os.path.exists(zip_file):\n",
        "        os.remove(zip_file)\n",
        "        print(f\"Removed {split}.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSyhVj2EeLcd"
      },
      "outputs": [],
      "source": [
        "# Class name to label index\n",
        "\n",
        "EVENT_DICTIONARY_action_class = {\"Tackling\":0,\"Standing tackling\":1,\"High leg\":2,\"Holding\":3,\"Pushing\":4,\n",
        "                        \"Elbowing\":5, \"Challenge\":6, \"Dive\":7, \"Dont know\":8}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_action_class = {0:\"Tackling\", 1:\"Standing tackling\", 2:\"High leg\", 3:\"Holding\", 4:\"Pushing\",\n",
        "                        5:\"Elbowing\", 6:\"Challenge\", 7:\"Dive\", 8:\"Dont know\"}\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY_offence_severity_class = {\"No offence\":0,\"Offence + No card\":1,\"Offence + Yellow card\":2,\"Offence + Red card\":3}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_offence_severity_class = {0:\"No offence\", 1:\"Offence + No card\", 2:\"Offence + Yellow card\", 3:\"Offence + Red card\"}\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY_offence_class = {\"Offence\":0,\"Between\":1,\"No Offence\":2, \"No offence\":2}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_offence_class = {0:\"Offence\", 1:\"Between\", 2:\"No Offence\"}\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY_severity_class = {\"1.0\":0,\"2.0\":1,\"3.0\":2,\"4.0\":3,\"5.0\":4}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_severity_class = {0:\"No card\", 1:\"Borderline No/Yellow\", 2:\"Yellow card\", 3:\"Borderline Yellow/Red\", 4:\"Red card\"}\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY_bodypart_class = {\"Upper body\":0,\"Under body\":1}\n",
        "\n",
        "INVERSE_EVENT_DICTIONARY_bodypart_class = {0:\"Upper body\", 1:\"Under body\"}\n",
        "\n",
        "\n",
        "\n",
        "EVENT_DICTIONARY = {'action_class':EVENT_DICTIONARY_action_class, 'offence_class': EVENT_DICTIONARY_offence_class,\n",
        "            'severity_class': EVENT_DICTIONARY_severity_class, 'bodypart_class': EVENT_DICTIONARY_bodypart_class, 'offence_severity_class': EVENT_DICTIONARY_offence_severity_class}\n",
        "INVERSE_EVENT_DICTIONARY = {'action_class':INVERSE_EVENT_DICTIONARY_action_class, 'offence_class': INVERSE_EVENT_DICTIONARY_offence_class,\n",
        "            'severity_class': INVERSE_EVENT_DICTIONARY_severity_class, 'bodypart_class': INVERSE_EVENT_DICTIONARY_bodypart_class, 'offence_severity_class': INVERSE_EVENT_DICTIONARY_offence_severity_class}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auXVbrHWeNap"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#DATA_PATH = 'path/to/SoccerNet/mvfouls'\n",
        "\n",
        "# Set the desired frame count\n",
        "DESIRED_FRAME_COUNT = 126\n",
        "\n",
        "# Load the EVENT_DICTIONARY for mapping annotation labels\n",
        "EVENT_DICTIONARY = {\n",
        "    'action_class': {\"Tackling\": 0, \"Standing tackling\": 1, \"High leg\": 2, \"Holding\": 3, \"Pushing\": 4,\n",
        "                     \"Elbowing\": 5, \"Challenge\": 6, \"Dive\": 7, \"Dont know\": 8},\n",
        "    'offence_class': {\"Offence\": 0, \"Between\": 1, \"No Offence\": 2, \"No offence\": 2},\n",
        "    'severity_class': {\"1.0\": 0, \"2.0\": 1, \"3.0\": 2, \"4.0\": 3, \"5.0\": 4},\n",
        "    'bodypart_class': {\"Upper body\": 0, \"Under body\": 1},\n",
        "    'offence_severity_class': {\"No offence\": 0, \"Offence + No card\": 1, \"Offence + Yellow card\": 2, \"Offence + Red card\": 3}\n",
        "}\n",
        "\n",
        "# Transformation for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((56, 56)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def load_filtered_clips_and_labels(DATA_PATH,split, max_samples):\n",
        "    clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = [], [], [], [], [], []\n",
        "\n",
        "    annotations_path = os.path.join(DATA_PATH, split, \"annotations.json\")\n",
        "    print(f\"Loading annotations from: {annotations_path}\")\n",
        "\n",
        "    with open(annotations_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "    print(f\"Total actions found in annotations: {len(annotations['Actions'])}\")\n",
        "\n",
        "    offence_count, no_offence_count, skipped_actions = 0, 0, 0\n",
        "    max_samples = max_samples  # Maximum samples for each class\n",
        "\n",
        "    for action_index, (action_key, action_data) in enumerate(annotations['Actions'].items()):\n",
        "        offence_class = action_data['Offence']\n",
        "\n",
        "        # Filter only 50 samples for each class\n",
        "        if (offence_class == \"Offence\" and offence_count >= max_samples) or \\\n",
        "           (offence_class in [\"No offence\", \"No Offence\"] and no_offence_count >= max_samples):\n",
        "            continue\n",
        "\n",
        "        action_class = action_data['Action class']\n",
        "        severity_class = action_data['Severity'] if action_data['Severity'].replace('.', '').isdigit() else '1.0'\n",
        "        bodypart_class = action_data.get('Bodypart', 'Upper body')\n",
        "        offence_severity = f\"{offence_class} + {EVENT_DICTIONARY['severity_class'].get(severity_class, 'No card')}\"\n",
        "\n",
        "        action_label = EVENT_DICTIONARY['action_class'].get(action_class)\n",
        "        offence_label = EVENT_DICTIONARY['offence_class'].get(offence_class)\n",
        "        severity_label = EVENT_DICTIONARY['severity_class'].get(severity_class)\n",
        "        bodypart_label = EVENT_DICTIONARY['bodypart_class'].get(bodypart_class)\n",
        "        offence_severity_label = EVENT_DICTIONARY['offence_severity_class'].get(offence_severity, 0)\n",
        "\n",
        "        if None in [action_label, offence_label, severity_label, bodypart_label, offence_severity_label]:\n",
        "            skipped_actions += 1\n",
        "            continue\n",
        "\n",
        "        action_folder = os.path.join(DATA_PATH, split, f\"action_{action_key}\")\n",
        "\n",
        "        if not os.path.exists(action_folder):\n",
        "            skipped_actions += 1\n",
        "            continue\n",
        "\n",
        "        action_clips = []\n",
        "        for clip_idx in range(2):\n",
        "            clip_path = os.path.join(action_folder, f\"clip_{clip_idx}.mp4\")\n",
        "            if not os.path.exists(clip_path):\n",
        "                continue\n",
        "\n",
        "            cap = cv2.VideoCapture(clip_path)\n",
        "            frames = []\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = Image.fromarray(frame)\n",
        "                frame = transform(frame)\n",
        "                frames.append(frame)\n",
        "            cap.release()\n",
        "\n",
        "            # Sample or pad frames to ensure uniform frame count\n",
        "            if len(frames) > DESIRED_FRAME_COUNT:\n",
        "                indices = np.linspace(0, len(frames) - 1, DESIRED_FRAME_COUNT).astype(int)\n",
        "                frames = [frames[i] for i in indices]\n",
        "            elif len(frames) < DESIRED_FRAME_COUNT:\n",
        "                frames += [frames[-1]] * (DESIRED_FRAME_COUNT - len(frames))\n",
        "\n",
        "            video_tensor = torch.stack(frames, dim=0)\n",
        "            action_clips.append(video_tensor)\n",
        "\n",
        "            # Print tensor shape and size for debugging\n",
        "            print(f\"Action {action_key}, Clip {clip_idx} tensor shape: {video_tensor.shape}\")\n",
        "            tensor_size = video_tensor.element_size() * video_tensor.nelement() / (1024**2)\n",
        "            print(f\"Tensor size: {tensor_size:.2f} MB\")\n",
        "\n",
        "        if action_clips:\n",
        "            clips.append(action_clips)\n",
        "            labels_action.append(action_label)\n",
        "            labels_offence.append(offence_label)\n",
        "            labels_severity.append(severity_label)\n",
        "            labels_bodypart.append(bodypart_label)\n",
        "            labels_offence_severity.append(offence_severity_label)\n",
        "\n",
        "            if offence_class == \"Offence\":\n",
        "                offence_count += 1\n",
        "            else:\n",
        "                no_offence_count += 1\n",
        "\n",
        "            print(f\"Added action {action_key} with {len(action_clips)} clips to dataset.\")\n",
        "\n",
        "        # Stop if we have 50 samples for each class\n",
        "        if offence_count >= max_samples and no_offence_count >= max_samples:\n",
        "            break\n",
        "\n",
        "    print(\"\\nSummary:\")\n",
        "    print(f\"Total actions loaded: {len(clips)}\")\n",
        "    print(f\"Total actions skipped: {skipped_actions}\")\n",
        "    if clips:\n",
        "        print(f\"Example clip shape: {clips[0][0].shape}\")\n",
        "        print(f\"First action label: {labels_action[0]}\")\n",
        "\n",
        "    return clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity\n",
        "\n",
        "\n",
        "# # Load datasets for each split\n",
        "# datasets = {}\n",
        "# for split in ['train', 'valid', 'test']:\n",
        "#     clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = load_filtered_clips_and_labels(DATA_PATH,split)\n",
        "#     datasets[split] = {\n",
        "#         'clips': clips,\n",
        "#         'labels': {\n",
        "#             'action': labels_action,\n",
        "#             'offence': labels_offence,\n",
        "#             'severity': labels_severity,\n",
        "#             'bodypart': labels_bodypart,\n",
        "#             'offence_severity': labels_offence_severity\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "# # Display dataset info for verification\n",
        "# for split in ['train', 'valid', 'test']:\n",
        "#     print(f\"{split.capitalize()} set:\")\n",
        "#     print(\"Number of samples:\", len(datasets[split]['clips']))\n",
        "#     print(\"Example label structure:\", datasets[split]['labels']['action'][0] if datasets[split]['labels']['action'] else \"No labels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "c1Kd9xSlePRJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorchvideo.models.hub import mvit_base_16x4\n",
        "import torchvision.transforms as T\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "class MViT(nn.Module):\n",
        "    def __init__(self, num_classes_action=9, num_classes_offence=3, num_classes_severity=5,\n",
        "                 num_classes_bodypart=2, num_classes_offence_severity=4, freeze_backbone=True):\n",
        "        super(MViT, self).__init__()\n",
        "\n",
        "        # Load pretrained MViT model\n",
        "        self.mvit = mvit_base_16x4(pretrained=True)\n",
        "\n",
        "        # Optionally freeze backbone layers\n",
        "        if freeze_backbone:\n",
        "            for param in self.mvit.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Define custom fully connected layers for each task\n",
        "        self.fc_action = nn.Linear(768, num_classes_action)\n",
        "        self.fc_offence = nn.Linear(768, num_classes_offence)\n",
        "        self.fc_severity = nn.Linear(768, num_classes_severity)\n",
        "        self.fc_bodypart = nn.Linear(768, num_classes_bodypart)\n",
        "        self.fc_offence_severity = nn.Linear(768, num_classes_offence_severity)\n",
        "\n",
        "        # Transformation for resizing input to match MViT expected input size\n",
        "        self.preprocess = T.Compose([\n",
        "            T.Resize((224, 224)),   # Resize spatial dimensions to 224x224\n",
        "            T.ConvertImageDtype(torch.float32)  # Convert to float\n",
        "        ])\n",
        "\n",
        "        # Global average pooling layer to reduce dimensions to [batch, 768]\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Limit the number of frames if necessary\n",
        "        max_frames = 16\n",
        "        if x.shape[1] > max_frames:\n",
        "            x = x[:, :max_frames, :, :, :]  # Truncate to max_frames along the temporal dimension\n",
        "\n",
        "        # Resize each frame to 224x224 and reshape to [batch, channels, frames, height, width]\n",
        "        x = torch.stack([self.preprocess(frame) for frame in x.unbind(dim=1)], dim=1)\n",
        "\n",
        "        # Pass through MViT backbone\n",
        "        x = self.mvit(x)  # Output shape should be [batch, 768, frames, height, width]\n",
        "\n",
        "        # Apply global average pooling to get [batch, 768]\n",
        "        x = self.global_avg_pool(x).view(x.size(0), -1)\n",
        "\n",
        "        # Forward through task-specific fully connected layers\n",
        "        action_out = self.fc_action(x)\n",
        "        offence_out = self.fc_offence(x)\n",
        "        severity_out = self.fc_severity(x)\n",
        "        bodypart_out = self.fc_bodypart(x)\n",
        "        offence_severity_out = self.fc_offence_severity(x)\n",
        "\n",
        "        return action_out, offence_out, severity_out, bodypart_out, offence_severity_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dcwls-UMeR3C"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# After each training epoch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4h3Is49leT01",
        "outputId": "bb4049b6-548e-43ce-863c-ce4e3a7a00d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading annotations from: path/to/SoccerNet/mvfouls/train/annotations.json\n",
            "Total actions found in annotations: 2916\n",
            "Action 0, Clip 0 tensor shape: torch.Size([126, 3, 56, 56])\n",
            "Tensor size: 4.52 MB\n",
            "Action 0, Clip 1 tensor shape: torch.Size([126, 3, 56, 56])\n",
            "Tensor size: 4.52 MB\n",
            "Added action 0 with 2 clips to dataset.\n",
            "Action 19, Clip 0 tensor shape: torch.Size([126, 3, 56, 56])\n",
            "Tensor size: 4.52 MB\n",
            "Action 19, Clip 1 tensor shape: torch.Size([126, 3, 56, 56])\n",
            "Tensor size: 4.52 MB\n",
            "Added action 19 with 2 clips to dataset.\n",
            "\n",
            "Summary:\n",
            "Total actions loaded: 2\n",
            "Total actions skipped: 0\n",
            "Example clip shape: torch.Size([126, 3, 56, 56])\n",
            "First action label: 6\n",
            "Loading annotations from: path/to/SoccerNet/mvfouls/valid/annotations.json\n",
            "Total actions found in annotations: 411\n",
            "Action 0, Clip 0 tensor shape: torch.Size([126, 3, 56, 56])\n",
            "Tensor size: 4.52 MB\n",
            "Action 0, Clip 1 tensor shape: torch.Size([126, 3, 56, 56])\n",
            "Tensor size: 4.52 MB\n",
            "Added action 0 with 2 clips to dataset.\n",
            "Action 2, Clip 0 tensor shape: torch.Size([126, 3, 56, 56])\n",
            "Tensor size: 4.52 MB\n",
            "Action 2, Clip 1 tensor shape: torch.Size([126, 3, 56, 56])\n",
            "Tensor size: 4.52 MB\n",
            "Added action 2 with 2 clips to dataset.\n",
            "\n",
            "Summary:\n",
            "Total actions loaded: 2\n",
            "Total actions skipped: 0\n",
            "Example clip shape: torch.Size([126, 3, 56, 56])\n",
            "First action label: 3\n",
            "Data loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 98.7MB/s]\n",
            "<ipython-input-29-799efb0f76b5>:94: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Initialize the gradient scaler\n",
            "<ipython-input-29-799efb0f76b5>:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "TwoStreamNetwork.forward() missing 1 required positional argument: 'flow_input'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-799efb0f76b5>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Forward pass with mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0maction_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffence_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbodypart_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffence_severity_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             loss = (criterion(action_out, action_labels) +\n\u001b[1;32m    116\u001b[0m                     \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffence_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffence_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TwoStreamNetwork.forward() missing 1 required positional argument: 'flow_input'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "#from model import MultiTaskActionRecognitionModel\n",
        "#from data import load_filtered_clips_and_labels\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define parameters\n",
        "batch_size = 1\n",
        "num_epochs = 10\n",
        "learning_rate = 0.0001\n",
        "max_samples = 1\n",
        "fixed_frame_count = 126  # Standardized frame count for all clips\n",
        "DATA_PATH = 'path/to/SoccerNet/mvfouls'\n",
        "\n",
        "# Transformation for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((56 , 56)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class ActionDataset(Dataset):\n",
        "    def __init__(self, clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity):\n",
        "        self.clips = clips\n",
        "        self.labels_action = labels_action\n",
        "        self.labels_offence = labels_offence\n",
        "        self.labels_severity = labels_severity\n",
        "        self.labels_bodypart = labels_bodypart\n",
        "        self.labels_offence_severity = labels_offence_severity\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clips)\n",
        "\n",
        "    def pad_or_truncate(self, clip):\n",
        "        # Pad or truncate each clip to fixed frame count\n",
        "        frames = []\n",
        "        for i, frame in enumerate(clip):\n",
        "            if not isinstance(frame, torch.Tensor):  # Ensure the frame is not already a tensor\n",
        "                frame = transform(frame)  # Apply transforms\n",
        "            frames.append(frame)\n",
        "            if len(frames) == fixed_frame_count:  # Truncate\n",
        "                break\n",
        "        # Pad if necessary\n",
        "        while len(frames) < fixed_frame_count:\n",
        "            frames.append(torch.zeros_like(frames[0]))  # Padding with empty frames\n",
        "        return torch.stack(frames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip = self.clips[idx]\n",
        "        padded_clip = self.pad_or_truncate(clip)\n",
        "        return (padded_clip,\n",
        "                self.labels_action[idx],\n",
        "                self.labels_offence[idx],\n",
        "                self.labels_severity[idx],\n",
        "                self.labels_bodypart[idx],\n",
        "                self.labels_offence_severity[idx])\n",
        "\n",
        "# Load filtered dataset\n",
        "train_clips, train_labels_action, train_labels_offence, train_labels_severity, train_labels_bodypart, train_labels_offence_severity = load_filtered_clips_and_labels(DATA_PATH, \"train\", max_samples=max_samples)\n",
        "valid_clips, valid_labels_action, valid_labels_offence, valid_labels_severity, valid_labels_bodypart, valid_labels_offence_severity = load_filtered_clips_and_labels(DATA_PATH,\"valid\", max_samples=max_samples)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = ActionDataset(train_clips, train_labels_action, train_labels_offence, train_labels_severity, train_labels_bodypart, train_labels_offence_severity)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_dataset = ActionDataset(valid_clips, valid_labels_action, valid_labels_offence, valid_labels_severity, valid_labels_bodypart, valid_labels_offence_severity)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "print('Data loaded')\n",
        "\n",
        "# Reshape the inputs to [batch_size, frames_clip_0 * frames_clip_1, channels, height, width]\n",
        "def reshape_tensor(x):\n",
        "    b, f0, f1, c, h, w = x.shape\n",
        "    return x.view(b, f0*f1, c, h, w).permute(0, 2, 1, 3, 4)\n",
        "\n",
        "# def reshape_tensor(x):\n",
        "#       return torch.einsum('bfgchw->bcfhw', x)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = MViT().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scaler = GradScaler()  # Initialize the gradient scaler\n",
        "accumulation_steps = 4  # Number of steps to accumulate gradients\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    optimizer.zero_grad()  # Reset gradients at the beginning of each epoch\n",
        "\n",
        "    for i, (inputs, action_labels, offence_labels, severity_labels, bodypart_labels, offence_severity_labels) in enumerate(train_loader):\n",
        "        inputs = reshape_tensor(inputs).to(device).to(torch.float32)  # Ensure inputs are float32\n",
        "\n",
        "        # Move labels to device\n",
        "        action_labels = action_labels.to(device)\n",
        "        offence_labels = offence_labels.to(device)\n",
        "        severity_labels = severity_labels.to(device)\n",
        "        bodypart_labels = bodypart_labels.to(device)\n",
        "        offence_severity_labels = offence_severity_labels.to(device)\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast():\n",
        "            action_out, offence_out, severity_out, bodypart_out, offence_severity_out = model(inputs)\n",
        "            loss = (criterion(action_out, action_labels) +\n",
        "                    criterion(offence_out, offence_labels) +\n",
        "                    criterion(severity_out, severity_labels) +\n",
        "                    criterion(bodypart_out, bodypart_labels) +\n",
        "                    criterion(offence_severity_out, offence_severity_labels))\n",
        "\n",
        "            # Scale loss and backpropagate\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "        # Update gradients only after accumulation steps\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()  # Reset gradients for the next accumulation\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad():  # Disable gradient tracking for validation\n",
        "        for inputs, action_labels, offence_labels, severity_labels, bodypart_labels, offence_severity_labels in valid_loader:\n",
        "            # Check the shape of inputs\n",
        "            #[batch_size, frames_clip_0, frames_clip_1, channels, height, width]\n",
        "            print(f\"Input shape before adjustment: {inputs.shape}\")  # Should print [2, 126, 126, 3, 64, 64] per action\n",
        "\n",
        "            # Reshape the inputs to [batch_size, frames_clip_0, frames_clip_1, channels, height, width]\n",
        "            #inputs = reshape_tensor(inputs)\n",
        "\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            action_labels = action_labels.to(device)\n",
        "            offence_labels = offence_labels.to(device)\n",
        "            severity_labels = severity_labels.to(device)\n",
        "            bodypart_labels = bodypart_labels.to(device)\n",
        "            offence_severity_labels = offence_severity_labels.to(device)\n",
        "\n",
        "            action_out, offence_out, severity_out, bodypart_out, offence_severity_out = model(inputs)\n",
        "\n",
        "            # Calculate validation loss\n",
        "            loss_action = criterion(action_out, action_labels)\n",
        "            loss_offence = criterion(offence_out, offence_labels)\n",
        "            loss_severity = criterion(severity_out, severity_labels)\n",
        "            loss_bodypart = criterion(bodypart_out, bodypart_labels)\n",
        "            loss_offence_severity = criterion(offence_severity_out, offence_severity_labels)\n",
        "\n",
        "            loss = loss_action + loss_offence + loss_severity + loss_bodypart + loss_offence_severity\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    print(f\"Validation Loss: {valid_loss / len(valid_loader):.4f}\")\n",
        "    # After each training epoch\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save the model\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"saved_models/action_recognition_model.pth\")\n",
        "print(\"Model saved!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
