{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQI8DJC8eFhF",
    "outputId": "b79607a9-aaa0-4e01-a8b9-7fef4b3286a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SoccerNet\n",
      "  Downloading SoccerNet-0.1.61-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (4.66.6)\n",
      "Collecting scikit-video (from SoccerNet)\n",
      "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (3.8.0)\n",
      "Collecting google-measurement-protocol (from SoccerNet)\n",
      "  Downloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl.metadata (845 bytes)\n",
      "Collecting pycocoevalcap (from SoccerNet)\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: huggingface-hub[cli] in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (0.24.7)\n",
      "Requirement already satisfied: requests<3.0a0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from google-measurement-protocol->SoccerNet) (2.32.3)\n",
      "Collecting prices>=1.0.0 (from google-measurement-protocol->SoccerNet)\n",
      "  Downloading prices-1.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (4.12.2)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli]->SoccerNet)\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet)\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (3.0.48)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.26.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (2.8.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap->SoccerNet) (2.0.8)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video->SoccerNet) (1.13.1)\n",
      "Requirement already satisfied: babel>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from prices>=1.0.0->google-measurement-protocol->SoccerNet) (2.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->SoccerNet) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2024.8.30)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (0.2.13)\n",
      "Downloading SoccerNet-0.1.61-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl (5.9 kB)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prices-1.1.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: prices, pfzy, scikit-video, InquirerPy, google-measurement-protocol, pycocoevalcap, SoccerNet\n",
      "Successfully installed InquirerPy-0.3.4 SoccerNet-0.1.61 google-measurement-protocol-1.1.0 pfzy-0.3.4 prices-1.1.1 pycocoevalcap-1.2 scikit-video-1.1.11\n"
     ]
    }
   ],
   "source": [
    "%pip install SoccerNet\n",
    "%pip install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsCegawieHO0",
    "outputId": "5fd7f5b7-fa0c-424a-bea5-9505f47dd64f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading path/to/SoccerNet/mvfouls/train.zip...:  14%|█▎        | 333M/2.46G [00:07<00:38, 54.7MiB/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from SoccerNet.Downloader import SoccerNetDownloader as SNdl\n",
    "\n",
    "# Set up the downloader\n",
    "local_directory = \"path/to/SoccerNet\"\n",
    "mySNdl = SNdl(LocalDirectory=local_directory)\n",
    "\n",
    "# Download the data\n",
    "mySNdl.downloadDataTask(task=\"mvfouls\", split=[\"train\", \"valid\", \"test\", \"challenge\"], password=\"s0cc3rn3t\")\n",
    "\n",
    "# Unzip the downloaded files\n",
    "task_directory = os.path.join(local_directory, \"mvfouls\")\n",
    "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
    "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
    "    if os.path.exists(zip_file):\n",
    "        # Create a new folder with the same name as the zip file\n",
    "        extract_folder = os.path.join(task_directory, split)\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "        # Extract the contents to the new folder\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder)\n",
    "        print(f\"Extracted {split}.zip to {extract_folder}\")\n",
    "    else:\n",
    "        print(f\"{split}.zip not found\")\n",
    "\n",
    "# Optionally, remove the zip files after extraction\n",
    "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
    "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
    "    if os.path.exists(zip_file):\n",
    "        os.remove(zip_file)\n",
    "        print(f\"Removed {split}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gSyhVj2EeLcd"
   },
   "outputs": [],
   "source": [
    "# Class name to label index\n",
    "\n",
    "EVENT_DICTIONARY_action_class = {\"Tackling\":0,\"Standing tackling\":1,\"High leg\":2,\"Holding\":3,\"Pushing\":4,\n",
    "                        \"Elbowing\":5, \"Challenge\":6, \"Dive\":7, \"Dont know\":8}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_action_class = {0:\"Tackling\", 1:\"Standing tackling\", 2:\"High leg\", 3:\"Holding\", 4:\"Pushing\",\n",
    "                        5:\"Elbowing\", 6:\"Challenge\", 7:\"Dive\", 8:\"Dont know\"}\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY_offence_severity_class = {\"No offence\":0,\"Offence + No card\":1,\"Offence + Yellow card\":2,\"Offence + Red card\":3}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_offence_severity_class = {0:\"No offence\", 1:\"Offence + No card\", 2:\"Offence + Yellow card\", 3:\"Offence + Red card\"}\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY_offence_class = {\"Offence\":0,\"Between\":1,\"No Offence\":2, \"No offence\":2}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_offence_class = {0:\"Offence\", 1:\"Between\", 2:\"No Offence\"}\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY_severity_class = {\"1.0\":0,\"2.0\":1,\"3.0\":2,\"4.0\":3,\"5.0\":4}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_severity_class = {0:\"No card\", 1:\"Borderline No/Yellow\", 2:\"Yellow card\", 3:\"Borderline Yellow/Red\", 4:\"Red card\"}\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY_bodypart_class = {\"Upper body\":0,\"Under body\":1}\n",
    "\n",
    "INVERSE_EVENT_DICTIONARY_bodypart_class = {0:\"Upper body\", 1:\"Under body\"}\n",
    "\n",
    "\n",
    "\n",
    "EVENT_DICTIONARY = {'action_class':EVENT_DICTIONARY_action_class, 'offence_class': EVENT_DICTIONARY_offence_class,\n",
    "            'severity_class': EVENT_DICTIONARY_severity_class, 'bodypart_class': EVENT_DICTIONARY_bodypart_class, 'offence_severity_class': EVENT_DICTIONARY_offence_severity_class}\n",
    "INVERSE_EVENT_DICTIONARY = {'action_class':INVERSE_EVENT_DICTIONARY_action_class, 'offence_class': INVERSE_EVENT_DICTIONARY_offence_class,\n",
    "            'severity_class': INVERSE_EVENT_DICTIONARY_severity_class, 'bodypart_class': INVERSE_EVENT_DICTIONARY_bodypart_class, 'offence_severity_class': INVERSE_EVENT_DICTIONARY_offence_severity_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "auXVbrHWeNap"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DATA_PATH = 'path/to/SoccerNet/mvfouls'\n",
    "\n",
    "# Set the desired frame count\n",
    "DESIRED_FRAME_COUNT = 126\n",
    "\n",
    "# Load the EVENT_DICTIONARY for mapping annotation labels\n",
    "EVENT_DICTIONARY = {\n",
    "    'action_class': {\"Tackling\": 0, \"Standing tackling\": 1, \"High leg\": 2, \"Holding\": 3, \"Pushing\": 4,\n",
    "                     \"Elbowing\": 5, \"Challenge\": 6, \"Dive\": 7, \"Dont know\": 8},\n",
    "    'offence_class': {\"Offence\": 0, \"Between\": 1, \"No Offence\": 2, \"No offence\": 2},\n",
    "    'severity_class': {\"1.0\": 0, \"2.0\": 1, \"3.0\": 2, \"4.0\": 3, \"5.0\": 4},\n",
    "    'bodypart_class': {\"Upper body\": 0, \"Under body\": 1},\n",
    "    'offence_severity_class': {\"No offence\": 0, \"Offence + No card\": 1, \"Offence + Yellow card\": 2, \"Offence + Red card\": 3}\n",
    "}\n",
    "\n",
    "# Transformation for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_filtered_clips_and_labels(DATA_PATH,split, max_samples):\n",
    "    clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = [], [], [], [], [], []\n",
    "\n",
    "    annotations_path = os.path.join(DATA_PATH, split, \"annotations.json\")\n",
    "    print(f\"Loading annotations from: {annotations_path}\")\n",
    "\n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    print(f\"Total actions found in annotations: {len(annotations['Actions'])}\")\n",
    "\n",
    "    offence_count, no_offence_count, skipped_actions = 0, 0, 0\n",
    "    max_samples = max_samples  # Maximum samples for each class\n",
    "\n",
    "    for action_index, (action_key, action_data) in enumerate(annotations['Actions'].items()):\n",
    "        offence_class = action_data['Offence']\n",
    "\n",
    "        # Filter only 50 samples for each class\n",
    "        if (offence_class == \"Offence\" and offence_count >= max_samples) or \\\n",
    "           (offence_class in [\"No offence\", \"No Offence\"] and no_offence_count >= max_samples):\n",
    "            continue\n",
    "\n",
    "        action_class = action_data['Action class']\n",
    "        severity_class = action_data['Severity'] if action_data['Severity'].replace('.', '').isdigit() else '1.0'\n",
    "        bodypart_class = action_data.get('Bodypart', 'Upper body')\n",
    "        offence_severity = f\"{offence_class} + {EVENT_DICTIONARY['severity_class'].get(severity_class, 'No card')}\"\n",
    "\n",
    "        action_label = EVENT_DICTIONARY['action_class'].get(action_class)\n",
    "        offence_label = EVENT_DICTIONARY['offence_class'].get(offence_class)\n",
    "        severity_label = EVENT_DICTIONARY['severity_class'].get(severity_class)\n",
    "        bodypart_label = EVENT_DICTIONARY['bodypart_class'].get(bodypart_class)\n",
    "        offence_severity_label = EVENT_DICTIONARY['offence_severity_class'].get(offence_severity, 0)\n",
    "\n",
    "        if None in [action_label, offence_label, severity_label, bodypart_label, offence_severity_label]:\n",
    "            skipped_actions += 1\n",
    "            continue\n",
    "\n",
    "        action_folder = os.path.join(DATA_PATH, split, f\"action_{action_key}\")\n",
    "\n",
    "        if not os.path.exists(action_folder):\n",
    "            skipped_actions += 1\n",
    "            continue\n",
    "\n",
    "        action_clips = []\n",
    "        for clip_idx in range(2):\n",
    "            clip_path = os.path.join(action_folder, f\"clip_{clip_idx}.mp4\")\n",
    "            if not os.path.exists(clip_path):\n",
    "                continue\n",
    "\n",
    "            cap = cv2.VideoCapture(clip_path)\n",
    "            frames = []\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)  # Convert NumPy array to PIL Image\n",
    "                frame = transform(frame)       # Apply the updated transform\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "\n",
    "            # Sample or pad frames to ensure uniform frame count\n",
    "            if len(frames) > DESIRED_FRAME_COUNT:\n",
    "                indices = np.linspace(0, len(frames) - 1, DESIRED_FRAME_COUNT).astype(int)\n",
    "                frames = [frames[i] for i in indices]\n",
    "            elif len(frames) < DESIRED_FRAME_COUNT:\n",
    "                frames += [frames[-1]] * (DESIRED_FRAME_COUNT - len(frames))\n",
    "\n",
    "            video_tensor = torch.stack(frames, dim=0)\n",
    "            action_clips.append(video_tensor)\n",
    "\n",
    "\n",
    "            # Print tensor shape and size for debugging\n",
    "            print(f\"Action {action_key}, Clip {clip_idx} tensor shape: {video_tensor.shape}\")\n",
    "            tensor_size = video_tensor.element_size() * video_tensor.nelement() / (1024**2)\n",
    "            print(f\"Tensor size: {tensor_size:.2f} MB\")\n",
    "\n",
    "        if action_clips:\n",
    "            clips.append(action_clips)\n",
    "            labels_action.append(action_label)\n",
    "            labels_offence.append(offence_label)\n",
    "            labels_severity.append(severity_label)\n",
    "            labels_bodypart.append(bodypart_label)\n",
    "            labels_offence_severity.append(offence_severity_label)\n",
    "\n",
    "            if offence_class == \"Offence\":\n",
    "                offence_count += 1\n",
    "            else:\n",
    "                no_offence_count += 1\n",
    "\n",
    "            print(f\"Added action {action_key} with {len(action_clips)} clips to dataset.\")\n",
    "\n",
    "        # Stop if we have 50 samples for each class\n",
    "        if offence_count >= max_samples and no_offence_count >= max_samples:\n",
    "            break\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total actions loaded: {len(clips)}\")\n",
    "    print(f\"Total actions skipped: {skipped_actions}\")\n",
    "    if clips:\n",
    "        print(f\"Example clip shape: {clips[0][0].shape}\")\n",
    "        print(f\"First action label: {labels_action[0]}\")\n",
    "\n",
    "    return clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity\n",
    "\n",
    "\n",
    "# # Load datasets for each split\n",
    "# datasets = {}\n",
    "# for split in ['train', 'valid', 'test']:\n",
    "#     clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = load_filtered_clips_and_labels(DATA_PATH,split)\n",
    "#     datasets[split] = {\n",
    "#         'clips': clips,\n",
    "#         'labels': {\n",
    "#             'action': labels_action,\n",
    "#             'offence': labels_offence,\n",
    "#             'severity': labels_severity,\n",
    "#             'bodypart': labels_bodypart,\n",
    "#             'offence_severity': labels_offence_severity\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "# # Display dataset info for verification\n",
    "# for split in ['train', 'valid', 'test']:\n",
    "#     print(f\"{split.capitalize()} set:\")\n",
    "#     print(\"Number of samples:\", len(datasets[split]['clips']))\n",
    "#     print(\"Example label structure:\", datasets[split]['labels']['action'][0] if datasets[split]['labels']['action'] else \"No labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations from: mvfouls\\train\\annotations.json\n",
      "Total actions found in annotations: 2916\n",
      "Action 0, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 0, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 0 with 2 clips to dataset.\n",
      "Action 1, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 1, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 1 with 2 clips to dataset.\n",
      "Action 2, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 2, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 2 with 2 clips to dataset.\n",
      "Action 3, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 3, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 3 with 2 clips to dataset.\n",
      "Action 4, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 4, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 4 with 2 clips to dataset.\n",
      "Action 19, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 19, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 19 with 2 clips to dataset.\n",
      "Action 25, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 25, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 25 with 2 clips to dataset.\n",
      "Action 32, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 32, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 32 with 2 clips to dataset.\n",
      "Action 33, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 33, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 33 with 2 clips to dataset.\n",
      "Action 38, Clip 0 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Action 38, Clip 1 tensor shape: torch.Size([126, 3, 224, 224])\n",
      "Tensor size: 72.35 MB\n",
      "Added action 38 with 2 clips to dataset.\n",
      "\n",
      "Summary:\n",
      "Total actions loaded: 10\n",
      "Total actions skipped: 0\n",
      "Example clip shape: torch.Size([126, 3, 224, 224])\n",
      "First action label: 6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 163\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[0;32m    162\u001b[0m model \u001b[38;5;241m=\u001b[39m MViTModel(num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\n\u001b[1;32m--> 163\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    166\u001b[0m evaluate_model(model, test_loader, DEVICE)\n",
      "Cell \u001b[1;32mIn[52], line 77\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, device, lr)\u001b[0m\n\u001b[0;32m     74\u001b[0m train_loss, train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     75\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m videos, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     78\u001b[0m     videos, labels \u001b[38;5;241m=\u001b[39m videos\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     79\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(videos)\n",
      "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\harka\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[52], line 26\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m     video \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(frame) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m video], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [T, C, H, W]\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Permute dimensions to [C, T, H, W] (required for Conv3D input)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m video, label\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorchvideo.models.hub import mvit_base_16x4\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, clips, labels, transform=None):\n",
    "        self.clips = clips\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = self.clips[idx]  # [T, C, H, W]\n",
    "        label = self.labels[idx]  # Scalar label\n",
    "\n",
    "        # Apply transformations to all frames\n",
    "        if self.transform:\n",
    "            video = torch.stack([self.transform(frame) for frame in video], dim=0)  # [T, C, H, W]\n",
    "\n",
    "        # Permute dimensions to [C, T, H, W] (required for Conv3D input)\n",
    "        video = video.permute(1, 0, 2, 3)\n",
    "\n",
    "        return video, label\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class MViTModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MViTModel, self).__init__()\n",
    "        self.mvit = mvit_base_16x4(pretrained=False)\n",
    "        self.mvit.head.proj = nn.Linear(self.mvit.head.proj.in_features, num_classes)  # Adjust final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mvit(x)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "def create_dataloaders(clips, labels, batch_size, transform, split_ratios=(0.7, 0.15, 0.15)):\n",
    "    # Split data into train, val, and test\n",
    "    data_size = len(clips)\n",
    "    train_size = int(split_ratios[0] * data_size)\n",
    "    val_size = int(split_ratios[1] * data_size)\n",
    "    test_size = data_size - train_size - val_size\n",
    "\n",
    "    train_clips, train_labels = clips[:train_size], labels[:train_size]\n",
    "    val_clips, val_labels = clips[train_size:train_size + val_size], labels[train_size:train_size + val_size]\n",
    "    test_clips, test_labels = clips[train_size + val_size:], labels[train_size + val_size:]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = VideoDataset(train_clips, train_labels, transform=transform)\n",
    "    val_dataset = VideoDataset(val_clips, val_labels, transform=transform)\n",
    "    test_dataset = VideoDataset(test_clips, test_labels, transform=transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for videos, labels in train_loader:\n",
    "            print(f\"Video batch shape: {videos.shape}\")  # Should be [batch_size, channels, depth, height, width]\n",
    "            print(f\"Labels shape: {labels.shape}\")  # Should be [batch_size]\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            outputs = model(videos)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * videos.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            total_samples += videos.size(0)\n",
    "\n",
    "        train_loss /= total_samples\n",
    "        train_accuracy = train_correct / total_samples\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for videos, labels in val_loader:\n",
    "                videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * videos.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                total_samples += videos.size(0)\n",
    "\n",
    "        val_loss /= total_samples\n",
    "        val_accuracy = val_correct / total_samples\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"    Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"    Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in test_loader:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(videos)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            total_samples += videos.size(0)\n",
    "\n",
    "    test_accuracy = test_correct / total_samples\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Example parameters\n",
    "    NUM_CLASSES = 9  # Adjust based on your dataset\n",
    "    BATCH_SIZE = 2\n",
    "    NUM_EPOCHS = 10\n",
    "    DATA_PATH = 'mvfouls'\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load your processed clips and labels\n",
    "    # Replace with your actual data\n",
    "    # Example: clips, labels = load_filtered_clips_and_labels(DATA_PATH, 'train', max_samples=50)\n",
    "    clips, labels_action, _, _, _, _ = load_filtered_clips_and_labels(DATA_PATH, 'train', max_samples=5)\n",
    "\n",
    "    labels = labels_action\n",
    "\n",
    "    # Transformations\n",
    "    transform = Compose([\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for pretrained models\n",
    "    ])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(clips, labels, BATCH_SIZE, transform)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = MViTModel(num_classes=NUM_CLASSES)\n",
    "    train_model(model, train_loader, val_loader, NUM_EPOCHS, DEVICE)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, test_loader, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
