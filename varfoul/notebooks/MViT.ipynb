{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributor - Jainil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the desired frame count\n",
    "DESIRED_FRAME_COUNT = 16\n",
    "\n",
    "# Load the EVENT_DICTIONARY for mapping annotation labels\n",
    "EVENT_DICTIONARY = {\n",
    "    'action_class': {\"Tackling\": 0, \"Standing tackling\": 1, \"High leg\": 2, \"Holding\": 3, \"Pushing\": 4,\n",
    "                     \"Elbowing\": 5, \"Challenge\": 6, \"Dive\": 7, \"Dont know\": 8},\n",
    "    'offence_class': {\"Offence\": 0, \"Between\": 1, \"No Offence\": 2, \"No offence\": 2},\n",
    "    'severity_class': {\"1.0\": 0, \"2.0\": 1, \"3.0\": 2, \"4.0\": 3, \"5.0\": 4},\n",
    "    'bodypart_class': {\"Upper body\": 0, \"Under body\": 1},\n",
    "    'offence_severity_class': {\"No offence\": 0, \"Offence + No card\": 1, \"Offence + Yellow card\": 2, \"Offence + Red card\": 3}\n",
    "}\n",
    "\n",
    "# Transformation for RGB preprocessing\n",
    "rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformation for flow preprocessing\n",
    "flow_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_filtered_clips_and_labels(DATA_PATH, split, max_samples_o, max_samples_no):\n",
    "    rgb_clips, flow_clips = [], []\n",
    "    labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = [], [], [], [], []\n",
    "\n",
    "    annotations_path = os.path.join(DATA_PATH, split, \"annotations.json\")\n",
    "    print(f\"Loading annotations from: {annotations_path}\")\n",
    "\n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    print(f\"Total actions found in annotations: {len(annotations['Actions'])}\")\n",
    "\n",
    "    offence_count, no_offence_count, skipped_actions = 0, 0, 0\n",
    "\n",
    "    for action_index, (action_key, action_data) in enumerate(annotations['Actions'].items()):\n",
    "        offence_class = action_data['Offence']\n",
    "        if (offence_class == \"Offence\" and offence_count >= max_samples_o) or \\\n",
    "           (offence_class in [\"No offence\", \"No Offence\"] and no_offence_count >= max_samples_no):\n",
    "            continue\n",
    "\n",
    "        # Map labels to indices using the dictionary\n",
    "        action_label = EVENT_DICTIONARY['action_class'].get(action_data['Action class'])\n",
    "        offence_label = EVENT_DICTIONARY['offence_class'].get(offence_class)\n",
    "        severity_label = EVENT_DICTIONARY['severity_class'].get(action_data.get('Severity', '1.0'))\n",
    "        bodypart_label = EVENT_DICTIONARY['bodypart_class'].get(action_data.get('Bodypart', 'Upper body'))\n",
    "        offence_severity = f\"{offence_class} + {EVENT_DICTIONARY['severity_class'].get(severity_label, 'No card')}\"\n",
    "        offence_severity_label = EVENT_DICTIONARY['offence_severity_class'].get(offence_severity, 0)\n",
    "\n",
    "        # Skip if any label is missing\n",
    "        if None in [action_label, offence_label, severity_label, bodypart_label, offence_severity_label]:\n",
    "            skipped_actions += 1\n",
    "            continue\n",
    "\n",
    "        action_folder = os.path.join(DATA_PATH, split, f\"action_{action_key}\")\n",
    "        if not os.path.exists(action_folder):\n",
    "            skipped_actions += 1\n",
    "            continue\n",
    "\n",
    "        rgb_action_clips, flow_action_clips = [], []\n",
    "        for clip_idx in range(2):\n",
    "            clip_path = os.path.join(action_folder, f\"clip_{clip_idx}.mp4\")\n",
    "            if not os.path.exists(clip_path):\n",
    "                continue\n",
    "\n",
    "            cap = cv2.VideoCapture(clip_path)\n",
    "            ret, prev_frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "            rgb_frames, flow_frames = [], []\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Process RGB frame\n",
    "                rgb_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                rgb_frame = rgb_transform(rgb_frame).to(device)\n",
    "                rgb_frames.append(rgb_frame)\n",
    "\n",
    "                # Process Optical Flow\n",
    "                curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                flow = np.clip(flow, -20, 20)  # Clipping to limit extreme values\n",
    "                flow = ((flow + 20) * (255.0 / 40)).astype(np.uint8)  # Normalizing to 0-255 range\n",
    "                flow_frame = Image.fromarray(flow[..., 0])  # Taking the horizontal component for simplicity\n",
    "                flow_frame = flow_transform(flow_frame).to(device)\n",
    "                flow_frames.append(flow_frame)\n",
    "                prev_gray = curr_gray\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "            # Adjust frame count\n",
    "            if len(rgb_frames) > DESIRED_FRAME_COUNT:\n",
    "                indices = np.linspace(0, len(rgb_frames) - 1, DESIRED_FRAME_COUNT).astype(int)\n",
    "                rgb_frames = [rgb_frames[i] for i in indices]\n",
    "                flow_frames = [flow_frames[i] for i in indices]\n",
    "            elif len(rgb_frames) < DESIRED_FRAME_COUNT:\n",
    "                rgb_frames += [rgb_frames[-1]] * (DESIRED_FRAME_COUNT - len(rgb_frames))\n",
    "                flow_frames += [flow_frames[-1]] * (DESIRED_FRAME_COUNT - len(flow_frames))\n",
    "\n",
    "            rgb_action_clips.append(torch.stack(rgb_frames, dim=0))\n",
    "            flow_action_clips.append(torch.stack(flow_frames, dim=0))\n",
    "\n",
    "        if rgb_action_clips and flow_action_clips:\n",
    "            rgb_clips.append(rgb_action_clips)\n",
    "            flow_clips.append(flow_action_clips)\n",
    "            labels_action.append(action_label)\n",
    "            labels_offence.append(offence_label)\n",
    "            labels_severity.append(severity_label)\n",
    "            labels_bodypart.append(bodypart_label)\n",
    "            labels_offence_severity.append(offence_severity_label)\n",
    "\n",
    "            if offence_class == \"Offence\":\n",
    "                offence_count += 1\n",
    "            else:\n",
    "                no_offence_count += 1\n",
    "\n",
    "        if offence_count >= max_samples_o and no_offence_count >= max_samples_no:\n",
    "            break\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total actions loaded: {len(rgb_clips)}\")\n",
    "    print(f\"Total actions skipped: {skipped_actions}\")\n",
    "    return rgb_clips, flow_clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MViTBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, \n",
    "                 dropout=0., attention_dropout=0., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiScaleAttention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, dropout=attention_dropout)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=112, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class MViTForFoulDetection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=112,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        dropout=0.1,\n",
    "        attention_dropout=0.1,\n",
    "        drop_path=0.1,\n",
    "        num_frames=8,\n",
    "        num_classes_action=9,\n",
    "        num_classes_offence=3,\n",
    "        num_classes_severity=5,\n",
    "        num_classes_bodypart=2,\n",
    "        num_classes_offence_severity=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_frames = num_frames\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "        \n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.temporal_embed = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, depth)]\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            MViTBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                dropout=dropout,\n",
    "                attention_dropout=attention_dropout,\n",
    "                drop_path=dpr[i]\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.fc_action = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.LayerNorm(embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim // 2, num_classes_action)\n",
    "        )\n",
    "        \n",
    "        self.fc_offence = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.LayerNorm(embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim // 2, num_classes_offence)\n",
    "        )\n",
    "        \n",
    "        self.fc_severity = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.LayerNorm(embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim // 2, num_classes_severity)\n",
    "        )\n",
    "        \n",
    "        self.fc_bodypart = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.LayerNorm(embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim // 2, num_classes_bodypart)\n",
    "        )\n",
    "        \n",
    "        self.fc_offence_severity = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.LayerNorm(embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim // 2, num_classes_offence_severity)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.temporal_embed, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # Reshape input: B, F, C, H, W -> (B * F), C, H, W\n",
    "        B = x.shape[0]\n",
    "        x = rearrange(x, 'b f c h w -> (b f) c h w')\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Reshape back: (B * F), N, D -> B, F, N, D\n",
    "        x = rearrange(x, '(b f) n d -> b f n d', b=B)\n",
    "        \n",
    "        # Add temporal embeddings\n",
    "        temporal_embed = repeat(self.temporal_embed, '1 f d -> b f d', b=B)\n",
    "        x = x + temporal_embed.unsqueeze(2)\n",
    "        \n",
    "        # Reshape for transformer blocks: B, F, N, D -> (B * F), N, D\n",
    "        x = rearrange(x, 'b f n d -> (b f) n d')\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed[:, 1:]\n",
    "        \n",
    "        # Add classification token\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=x.shape[0])\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Take only the classification token output\n",
    "        x = x[:, 0]\n",
    "        \n",
    "        # Reshape back to batch dimension: (B * F), D -> B, F, D\n",
    "        x = rearrange(x, '(b f) d -> b f d', b=B)\n",
    "        \n",
    "        # Global temporal pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        \n",
    "        return {\n",
    "            'action': self.fc_action(x),\n",
    "            'offence': self.fc_offence(x),\n",
    "            'severity': self.fc_severity(x),\n",
    "            'bodypart': self.fc_bodypart(x),\n",
    "            'offence_severity': self.fc_offence_severity(x)\n",
    "        }\n",
    "\n",
    "# Training utilities\n",
    "class FoulDetectionLoss(nn.Module):\n",
    "    def __init__(self, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        losses = {}\n",
    "        \n",
    "        for task, pred in predictions.items():\n",
    "            if self.class_weights is not None and task in self.class_weights:\n",
    "                weight = self.class_weights[task].to(pred.device)\n",
    "                losses[task] = F.cross_entropy(pred, targets[task], weight=weight)\n",
    "            else:\n",
    "                losses[task] = F.cross_entropy(pred, targets[task])\n",
    "        \n",
    "        # Total loss is the sum of all task losses\n",
    "        total_loss = sum(losses.values())\n",
    "        return total_loss, losses\n",
    "\n",
    "def create_mvit_model(num_frames=16):\n",
    "    model = MViTForFoulDetection(\n",
    "        img_size=112,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        num_frames=num_frames,\n",
    "        dropout=0.1,\n",
    "        attention_dropout=0.1,\n",
    "        drop_path=0.1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#from mvit_model import MViTForFoulDetection, FoulDetectionLoss  # Import from our previous implementation\n",
    "\n",
    "class ActionDataset(Dataset):\n",
    "    def __init__(self, rgb_clips, flow_clips, labels, transform=None, num_frames=8):\n",
    "        self.rgb_clips = rgb_clips\n",
    "        self.flow_clips = flow_clips\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_clips)\n",
    "\n",
    "    def sample_frames(self, clip):\n",
    "        # Uniformly sample frames if clip length > num_frames\n",
    "        if len(clip) > self.num_frames:\n",
    "            indices = np.linspace(0, len(clip)-1, self.num_frames, dtype=int)\n",
    "            return [clip[i] for i in indices]\n",
    "        # Repeat last frame if clip length < num_frames\n",
    "        while len(clip) < self.num_frames:\n",
    "            clip.append(clip[-1])\n",
    "        return clip\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Sample frames\n",
    "        rgb_frames = self.sample_frames(self.rgb_clips[idx])\n",
    "        flow_frames = self.sample_frames(self.flow_clips[idx])\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            rgb_frames = [self.transform(frame) if not isinstance(frame, torch.Tensor) else frame \n",
    "                         for frame in rgb_frames]\n",
    "            flow_frames = [self.transform(frame) if not isinstance(frame, torch.Tensor) else frame \n",
    "                         for frame in flow_frames]\n",
    "\n",
    "        # Stack frames\n",
    "        rgb_frames = torch.stack(rgb_frames, dim=0)  # [num_frames, C, H, W]\n",
    "        flow_frames = torch.stack(flow_frames, dim=0)  # [num_frames, C, H, W]\n",
    "\n",
    "        # Create label dictionary\n",
    "        label_dict = {key: torch.tensor(self.labels[key][idx], dtype=torch.long) \n",
    "                     for key in self.labels.keys()}\n",
    "\n",
    "        return rgb_frames, flow_frames, label_dict\n",
    "\n",
    "class MViTTrainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, criterion, optimizer, scheduler, device, num_epochs, grad_clip_val=1.0, mixed_precision=True):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.grad_clip_val = grad_clip_val\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.scaler = GradScaler() if mixed_precision else None\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = {key: [] for key in ['action', 'offence', 'severity', 'bodypart', 'offence_severity']}\n",
    "        all_labels = {key: [] for key in all_preds.keys()}\n",
    "\n",
    "        for batch_idx, (rgb_input, flow_input, labels) in enumerate(tqdm(self.train_loader)):\n",
    "            rgb_input = rgb_input.to(self.device)\n",
    "            flow_input = flow_input.to(self.device)\n",
    "            labels = {k: v.to(self.device) for k, v in labels.items()}\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with autocast(enabled=self.mixed_precision):\n",
    "                outputs = self.model(rgb_input)\n",
    "                loss, task_losses = self.criterion(outputs, labels)\n",
    "\n",
    "            if self.mixed_precision:\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_val)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_val)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            for task in all_preds.keys():\n",
    "                all_preds[task].extend(outputs[task].argmax(dim=1).cpu().numpy())\n",
    "                all_labels[task].extend(labels[task].cpu().numpy())\n",
    "\n",
    "            # Print batch metrics every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "                for task, l in task_losses.items():\n",
    "                    print(f\"{task}_loss = {l.item():.4f}\")\n",
    "\n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        accuracies = {task: accuracy_score(all_labels[task], all_preds[task]) for task in all_preds.keys()}\n",
    "        return avg_loss, accuracies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        # validate method remains unchanged\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_preds = {key: [] for key in ['action', 'offence', 'severity', 'bodypart', 'offence_severity']}\n",
    "        all_labels = {key: [] for key in all_preds.keys()}\n",
    "\n",
    "        for rgb_input, flow_input, labels in tqdm(self.valid_loader):\n",
    "            rgb_input = rgb_input.to(self.device)\n",
    "            flow_input = flow_input.to(self.device)\n",
    "            labels = {k: v.to(self.device) for k, v in labels.items()}\n",
    "\n",
    "            outputs = self.model(rgb_input)\n",
    "            loss, task_losses = self.criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            for task in all_preds.keys():\n",
    "                all_preds[task].extend(outputs[task].argmax(dim=1).cpu().numpy())\n",
    "                all_labels[task].extend(labels[task].cpu().numpy())\n",
    "\n",
    "        avg_loss = running_loss / len(self.valid_loader)\n",
    "        accuracies = {task: accuracy_score(all_labels[task], all_preds[task]) \n",
    "                     for task in all_preds.keys()}\n",
    "\n",
    "        return avg_loss, accuracies\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.num_epochs}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_accuracies = self.train_one_epoch()\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_accuracies = self.validate()\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Accuracies: {train_accuracies}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Accuracies: {val_accuracies}\")\n",
    "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                }, \"best_mvit_model.pth\")\n",
    "                print(\"Saved best model.\")\n",
    "\n",
    "def main(data_path, num_epochs=5, batch_size=2, learning_rate=5e-5, num_frames=8, max_samples_o=2, max_samples_no=2):\n",
    "    # Set device and enable mixed precision training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train_rgb_clips, train_flow_clips, train_labels_action, train_labels_offence, \\\n",
    "    train_labels_severity, train_labels_bodypart, train_labels_offence_severity = \\\n",
    "    load_filtered_clips_and_labels(data_path, \"train\", max_samples_o, max_samples_no)\n",
    "\n",
    "    valid_rgb_clips, valid_flow_clips, valid_labels_action, valid_labels_offence, \\\n",
    "    valid_labels_severity, valid_labels_bodypart, valid_labels_offence_severity = \\\n",
    "    load_filtered_clips_and_labels(data_path, \"valid\", max_samples_o, max_samples_no)\n",
    "\n",
    "    # Rest of the main function remains unchanged, just remove wandb.init() and wandb.finish()\n",
    "    # ...\n",
    "    # Set device and enable mixed precision training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_rgb_clips, train_flow_clips, train_labels_action, train_labels_offence, \\\n",
    "    train_labels_severity, train_labels_bodypart, train_labels_offence_severity = \\\n",
    "        load_filtered_clips_and_labels(data_path, \"train\", max_samples_o, max_samples_no)\n",
    "    \n",
    "    valid_rgb_clips, valid_flow_clips, valid_labels_action, valid_labels_offence, \\\n",
    "    valid_labels_severity, valid_labels_bodypart, valid_labels_offence_severity = \\\n",
    "        load_filtered_clips_and_labels(data_path, \"valid\", max_samples_o, max_samples_no)\n",
    "\n",
    "    # Organize labels\n",
    "    train_labels = {\n",
    "        \"action\": train_labels_action,\n",
    "        \"offence\": train_labels_offence,\n",
    "        \"severity\": train_labels_severity,\n",
    "        \"bodypart\": train_labels_bodypart,\n",
    "        \"offence_severity\": train_labels_offence_severity\n",
    "    }\n",
    "    valid_labels = {\n",
    "        \"action\": valid_labels_action,\n",
    "        \"offence\": valid_labels_offence,\n",
    "        \"severity\": valid_labels_severity,\n",
    "        \"bodypart\": valid_labels_bodypart,\n",
    "        \"offence_severity\": valid_labels_offence_severity\n",
    "    }\n",
    "\n",
    "    # Define transforms with additional augmentation for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ActionDataset(train_rgb_clips, train_flow_clips, train_labels, \n",
    "                                transform=train_transform, num_frames=num_frames)\n",
    "    valid_dataset = ActionDataset(valid_rgb_clips, valid_flow_clips, valid_labels, \n",
    "                                transform=valid_transform, num_frames=num_frames)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                            num_workers=4, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Initialize model and move to device\n",
    "    model = MViTForFoulDetection(num_frames=num_frames).to(device)\n",
    "    \n",
    "    # Initialize criterion with class weights\n",
    "    criterion = FoulDetectionLoss()\n",
    "    \n",
    "    # Initialize optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, \n",
    "                           weight_decay=0.05, betas=(0.9, 0.999))\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    # Create trainer instance\n",
    "    trainer = MViTTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        grad_clip_val=1.0,\n",
    "        mixed_precision=True\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = 'mvfouls'\n",
    "    main(data_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
