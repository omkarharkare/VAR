{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQI8DJC8eFhF",
    "outputId": "b79607a9-aaa0-4e01-a8b9-7fef4b3286a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SoccerNet\n",
      "  Downloading SoccerNet-0.1.61-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (4.66.6)\n",
      "Collecting scikit-video (from SoccerNet)\n",
      "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (3.8.0)\n",
      "Collecting google-measurement-protocol (from SoccerNet)\n",
      "  Downloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl.metadata (845 bytes)\n",
      "Collecting pycocoevalcap (from SoccerNet)\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: huggingface-hub[cli] in /usr/local/lib/python3.10/dist-packages (from SoccerNet) (0.24.7)\n",
      "Requirement already satisfied: requests<3.0a0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from google-measurement-protocol->SoccerNet) (2.32.3)\n",
      "Collecting prices>=1.0.0 (from google-measurement-protocol->SoccerNet)\n",
      "  Downloading prices-1.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]->SoccerNet) (4.12.2)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli]->SoccerNet)\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet)\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (3.0.48)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (1.26.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SoccerNet) (2.8.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap->SoccerNet) (2.0.8)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video->SoccerNet) (1.13.1)\n",
      "Requirement already satisfied: babel>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from prices>=1.0.0->google-measurement-protocol->SoccerNet) (2.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->SoccerNet) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2024.8.30)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli]->SoccerNet) (0.2.13)\n",
      "Downloading SoccerNet-0.1.61-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl (5.9 kB)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prices-1.1.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: prices, pfzy, scikit-video, InquirerPy, google-measurement-protocol, pycocoevalcap, SoccerNet\n",
      "Successfully installed InquirerPy-0.3.4 SoccerNet-0.1.61 google-measurement-protocol-1.1.0 pfzy-0.3.4 prices-1.1.1 pycocoevalcap-1.2 scikit-video-1.1.11\n"
     ]
    }
   ],
   "source": [
    "%pip install SoccerNet\n",
    "%pip install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsCegawieHO0",
    "outputId": "5fd7f5b7-fa0c-424a-bea5-9505f47dd64f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading path/to/SoccerNet/mvfouls/train.zip...:  14%|█▎        | 333M/2.46G [00:07<00:38, 54.7MiB/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from SoccerNet.Downloader import SoccerNetDownloader as SNdl\n",
    "\n",
    "# Set up the downloader\n",
    "local_directory = \"path/to/SoccerNet\"\n",
    "mySNdl = SNdl(LocalDirectory=local_directory)\n",
    "\n",
    "# Download the data\n",
    "mySNdl.downloadDataTask(task=\"mvfouls\", split=[\"train\", \"valid\", \"test\", \"challenge\"], password=\"pass\")\n",
    "\n",
    "# Unzip the downloaded files\n",
    "task_directory = os.path.join(local_directory, \"mvfouls\")\n",
    "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
    "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
    "    if os.path.exists(zip_file):\n",
    "        # Create a new folder with the same name as the zip file\n",
    "        extract_folder = os.path.join(task_directory, split)\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "        # Extract the contents to the new folder\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_folder)\n",
    "        print(f\"Extracted {split}.zip to {extract_folder}\")\n",
    "    else:\n",
    "        print(f\"{split}.zip not found\")\n",
    "\n",
    "# Optionally, remove the zip files after extraction\n",
    "for split in [\"train\", \"valid\", \"test\", \"challenge\"]:\n",
    "    zip_file = os.path.join(task_directory, f\"{split}.zip\")\n",
    "    if os.path.exists(zip_file):\n",
    "        os.remove(zip_file)\n",
    "        print(f\"Removed {split}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the desired frame count\n",
    "DESIRED_FRAME_COUNT = 126\n",
    "\n",
    "# Load the EVENT_DICTIONARY for mapping annotation labels\n",
    "EVENT_DICTIONARY = {\n",
    "    'action_class': {\"Tackling\": 0, \"Standing tackling\": 1, \"High leg\": 2, \"Holding\": 3, \"Pushing\": 4,\n",
    "                     \"Elbowing\": 5, \"Challenge\": 6, \"Dive\": 7, \"Dont know\": 8},\n",
    "    'offence_class': {\"Offence\": 0, \"Between\": 1, \"No Offence\": 2, \"No offence\": 2},\n",
    "    'severity_class': {\"1.0\": 0, \"2.0\": 1, \"3.0\": 2, \"4.0\": 3, \"5.0\": 4},\n",
    "    'bodypart_class': {\"Upper body\": 0, \"Under body\": 1},\n",
    "    'offence_severity_class': {\"No offence\": 0, \"Offence + No card\": 1, \"Offence + Yellow card\": 2, \"Offence + Red card\": 3}\n",
    "}\n",
    "\n",
    "# Transformation for RGB preprocessing (resize to 224x224 for MViT)\n",
    "rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformation for optical flow preprocessing (resize to 224x224 for MViT)\n",
    "flow_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_filtered_clips_and_labels(DATA_PATH, split, max_samples_o, max_samples_no):\n",
    "    rgb_clips, flow_clips = [], []\n",
    "    labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity = [], [], [], [], []\n",
    "\n",
    "    annotations_path = os.path.join(DATA_PATH, split, \"annotations.json\")\n",
    "    print(f\"Loading annotations from: {annotations_path}\")\n",
    "\n",
    "    with open(annotations_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    print(f\"Total actions found in annotations: {len(annotations['Actions'])}\")\n",
    "\n",
    "    offence_count, no_offence_count, skipped_actions = 0, 0, 0\n",
    "\n",
    "    for action_index, (action_key, action_data) in enumerate(annotations['Actions'].items()):\n",
    "        offence_class = action_data['Offence']\n",
    "        if (offence_class == \"Offence\" and offence_count >= max_samples_o) or \\\n",
    "           (offence_class in [\"No offence\", \"No Offence\"] and no_offence_count >= max_samples_no):\n",
    "            continue\n",
    "\n",
    "        # Map labels to indices using the dictionary\n",
    "        action_label = EVENT_DICTIONARY['action_class'].get(action_data['Action class'])\n",
    "        offence_label = EVENT_DICTIONARY['offence_class'].get(offence_class)\n",
    "        severity_label = EVENT_DICTIONARY['severity_class'].get(action_data.get('Severity', '1.0'))\n",
    "        bodypart_label = EVENT_DICTIONARY['bodypart_class'].get(action_data.get('Bodypart', 'Upper body'))\n",
    "        offence_severity = f\"{offence_class} + {EVENT_DICTIONARY['severity_class'].get(severity_label, 'No card')}\"\n",
    "        offence_severity_label = EVENT_DICTIONARY['offence_severity_class'].get(offence_severity, 0)\n",
    "\n",
    "        # Skip if any label is missing\n",
    "        if None in [action_label, offence_label, severity_label, bodypart_label, offence_severity_label]:\n",
    "            skipped_actions += 1\n",
    "            continue\n",
    "\n",
    "        action_folder = os.path.join(DATA_PATH, split, f\"action_{action_key}\")\n",
    "        if not os.path.exists(action_folder):\n",
    "            skipped_actions += 1\n",
    "            continue\n",
    "\n",
    "        rgb_action_clips, flow_action_clips = [], []\n",
    "        for clip_idx in range(2):\n",
    "            clip_path = os.path.join(action_folder, f\"clip_{clip_idx}.mp4\")\n",
    "            if not os.path.exists(clip_path):\n",
    "                continue\n",
    "\n",
    "            cap = cv2.VideoCapture(clip_path)\n",
    "            ret, prev_frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "            rgb_frames, flow_frames = [], []\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Process RGB frame\n",
    "                rgb_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                rgb_frame = rgb_transform(rgb_frame).to(device)\n",
    "                rgb_frames.append(rgb_frame)\n",
    "\n",
    "                # Process Optical Flow\n",
    "                curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                flow = np.clip(flow, -20, 20)  # Clipping to limit extreme values\n",
    "                flow = ((flow + 20) * (255.0 / 40)).astype(np.uint8)  # Normalizing to 0-255 range\n",
    "                flow_frame = Image.fromarray(flow[..., 0])  # Taking the horizontal component for simplicity\n",
    "                flow_frame = flow_transform(flow_frame).to(device)\n",
    "                flow_frames.append(flow_frame)\n",
    "                prev_gray = curr_gray\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "            # Adjust frame count\n",
    "            if len(rgb_frames) > DESIRED_FRAME_COUNT:\n",
    "                indices = np.linspace(0, len(rgb_frames) - 1, DESIRED_FRAME_COUNT).astype(int)\n",
    "                rgb_frames = [rgb_frames[i] for i in indices]\n",
    "                flow_frames = [flow_frames[i] for i in indices]\n",
    "            elif len(rgb_frames) < DESIRED_FRAME_COUNT:\n",
    "                rgb_frames += [rgb_frames[-1]] * (DESIRED_FRAME_COUNT - len(rgb_frames))\n",
    "                flow_frames += [flow_frames[-1]] * (DESIRED_FRAME_COUNT - len(flow_frames))\n",
    "\n",
    "            rgb_action_clips.append(torch.stack(rgb_frames, dim=0))\n",
    "            flow_action_clips.append(torch.stack(flow_frames, dim=0))\n",
    "\n",
    "        if rgb_action_clips and flow_action_clips:\n",
    "            rgb_clips.append(rgb_action_clips)\n",
    "            flow_clips.append(flow_action_clips)\n",
    "            labels_action.append(action_label)\n",
    "            labels_offence.append(offence_label)\n",
    "            labels_severity.append(severity_label)\n",
    "            labels_bodypart.append(bodypart_label)\n",
    "            labels_offence_severity.append(offence_severity_label)\n",
    "\n",
    "            if offence_class == \"Offence\":\n",
    "                offence_count += 1\n",
    "            else:\n",
    "                no_offence_count += 1\n",
    "\n",
    "        if offence_count >= max_samples_o and no_offence_count >= max_samples_no:\n",
    "            break\n",
    "\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total actions loaded: {len(rgb_clips)}\")\n",
    "    print(f\"Total actions skipped: {skipped_actions}\")\n",
    "    return rgb_clips, flow_clips, labels_action, labels_offence, labels_severity, labels_bodypart, labels_offence_severity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auXVbrHWeNap"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ImprovedTwoStreamNetwork(nn.Module):\n",
    "    def __init__(self, num_classes_action=9, num_classes_offence=3, num_classes_severity=5,\n",
    "                 num_classes_bodypart=2, num_classes_offence_severity=4, freeze_backbone=True):\n",
    "        super(ImprovedTwoStreamNetwork, self).__init__()\n",
    "\n",
    "        # Load more advanced backbones\n",
    "        # Using RegNet-Y as it shows better performance than ResNet\n",
    "        self.rgb_backbone = models.regnet_y_32gf(weights='IMAGENET1K_V2')\n",
    "        self.flow_backbone = models.regnet_y_32gf(weights='IMAGENET1K_V2')\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.rgb_backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.flow_backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        num_ftrs = self.rgb_backbone.fc.in_features\n",
    "\n",
    "        # Replace final classification layers with Identity\n",
    "        self.rgb_backbone.fc = nn.Identity()\n",
    "        self.flow_backbone.fc = nn.Identity()\n",
    "\n",
    "        # Temporal attention mechanism using Transformer Encoder\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model=num_ftrs,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.temporal_encoder = TransformerEncoder(encoder_layers, num_layers=2)\n",
    "\n",
    "        # Stream fusion module\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(num_ftrs * 2, num_ftrs),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_ftrs, num_ftrs)\n",
    "        )\n",
    "\n",
    "        # Task-specific heads with deeper architecture\n",
    "        self.fc_action = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes_action)\n",
    "        )\n",
    "\n",
    "        self.fc_offence = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes_offence)\n",
    "        )\n",
    "\n",
    "        self.fc_severity = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes_severity)\n",
    "        )\n",
    "\n",
    "        self.fc_bodypart = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes_bodypart)\n",
    "        )\n",
    "\n",
    "        self.fc_offence_severity = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes_offence_severity)\n",
    "        )\n",
    "\n",
    "    def temporal_attention(self, features):\n",
    "        \"\"\"\n",
    "        Apply scaled dot-product attention over time.\n",
    "        \n",
    "        Args:\n",
    "          features: Tensor of shape [batch_size * num_streams x frames x feature_dim]\n",
    "        \n",
    "        Returns:\n",
    "          Attended features of shape [batch_size * num_streams x frames x feature_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attention_weights = torch.matmul(features, features.transpose(-2, -1)) / features.size(-1)**0.5\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to the features\n",
    "        attended_features = torch.matmul(attention_weights, features)\n",
    "        \n",
    "        return attended_features\n",
    "\n",
    "    def forward(self, rgb_input, flow_input):\n",
    "        batch_size, num_streams, num_frames, _, _, _ = rgb_input.shape\n",
    "\n",
    "        # Reshape inputs for conv2d (merge batch_size * streams * frames into one dimension)\n",
    "        rgb_input = rgb_input.view(batch_size * num_streams * num_frames, 3, 112, 112)\n",
    "        flow_input = flow_input.view(batch_size * num_streams * num_frames, 1, 112, 112)\n",
    "\n",
    "        # Repeat flow_input across channels to match RGB input dimensions (if needed)\n",
    "        flow_input = flow_input.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Pass inputs through respective backbones (feature extraction)\n",
    "        rgb_features = self.rgb_backbone(rgb_input)   # Shape: [batch_size * streams * frames x feature_dim]\n",
    "        flow_features = self.flow_backbone(flow_input)\n",
    "\n",
    "        # Reshape features back to [batch_size * streams x frames x feature_dim]\n",
    "        rgb_features = rgb_features.view(batch_size * num_streams, num_frames, -1)\n",
    "        flow_features = flow_features.view(batch_size * num_streams, num_frames, -1)\n",
    "\n",
    "        # Apply temporal attention and transformer encoding\n",
    "        rgb_features = self.temporal_encoder(rgb_features)\n",
    "        flow_features = self.temporal_encoder(flow_features)\n",
    "\n",
    "        # Apply temporal attention (scaled dot-product attention)\n",
    "        rgb_features = self.temporal_attention(rgb_features)\n",
    "        flow_features = self.temporal_attention(flow_features)\n",
    "\n",
    "        # Global average pooling over frames\n",
    "        rgb_features = rgb_features.mean(dim=1)  # Shape: [batch_size * streams x feature_dim]\n",
    "        flow_features = flow_features.mean(dim=1)\n",
    "\n",
    "        # Reshape to [batch_size x streams x features]\n",
    "        rgb_features = rgb_features.view(batch_size, num_streams, -1)\n",
    "        flow_features = flow_features.view(batch_size, num_streams, -1)\n",
    "\n",
    "        # Concatenate and fuse streams\n",
    "        combined_features = torch.cat((rgb_features, flow_features), dim=-1)\n",
    "        \n",
    "        combined_features = self.fusion_layer(combined_features)\n",
    "\n",
    "        # Average across streams (if multiple views exist per action)\n",
    "        combined_features = combined_features.mean(dim=1)\n",
    "\n",
    "        # Forward through task-specific layers for multi-task learning\n",
    "        action_out = self.fc_action(combined_features)\n",
    "        offence_out = self.fc_offence(combined_features)\n",
    "        severity_out = self.fc_severity(combined_features)\n",
    "        bodypart_out = self.fc_bodypart(combined_features)\n",
    "        offence_severity_out = self.fc_offence_severity(combined_features)\n",
    "\n",
    "        return action_out, offence_out, severity_out, bodypart_out, offence_severity_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations from: mvfouls\\train\\annotations.json\n",
      "Total actions found in annotations: 2916\n",
      "\n",
      "Summary:\n",
      "Total actions loaded: 2\n",
      "Total actions skipped: 6\n",
      "Loading annotations from: mvfouls\\valid\\annotations.json\n",
      "Total actions found in annotations: 411\n",
      "\n",
      "Summary:\n",
      "Total actions loaded: 2\n",
      "Total actions skipped: 0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.1290 | Train Accuracies: {'action': 0.0, 'offence': 0.0, 'severity': 0.5, 'bodypart': 0.5, 'offence_severity': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 6.2780 | Val Accuracies: {'action': 0.5, 'offence': 0.5, 'severity': 0.5, 'bodypart': 1.0, 'offence_severity': 0.5}\n",
      "Saved best model.\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4933 | Train Accuracies: {'action': 0.0, 'offence': 0.0, 'severity': 1.0, 'bodypart': 0.0, 'offence_severity': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 6.3634 | Val Accuracies: {'action': 0.0, 'offence': 0.5, 'severity': 0.5, 'bodypart': 0.0, 'offence_severity': 0.5}\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.2114 | Train Accuracies: {'action': 0.5, 'offence': 0.5, 'severity': 1.0, 'bodypart': 0.5, 'offence_severity': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 7.3725 | Val Accuracies: {'action': 0.0, 'offence': 0.5, 'severity': 0.5, 'bodypart': 0.0, 'offence_severity': 0.5}\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.7978 | Train Accuracies: {'action': 0.0, 'offence': 0.0, 'severity': 1.0, 'bodypart': 0.0, 'offence_severity': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 8.3225 | Val Accuracies: {'action': 0.5, 'offence': 0.5, 'severity': 0.5, 'bodypart': 1.0, 'offence_severity': 0.5}\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.2467 | Train Accuracies: {'action': 0.5, 'offence': 0.5, 'severity': 1.0, 'bodypart': 0.5, 'offence_severity': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 9.5705 | Val Accuracies: {'action': 0.0, 'offence': 0.5, 'severity': 0.5, 'bodypart': 0.0, 'offence_severity': 0.5}\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7976 | Train Accuracies: {'action': 0.5, 'offence': 0.5, 'severity': 1.0, 'bodypart': 0.5, 'offence_severity': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 10.5432 | Val Accuracies: {'action': 0.0, 'offence': 0.5, 'severity': 0.5, 'bodypart': 0.0, 'offence_severity': 0.5}\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.3452 | Train Accuracies: {'action': 0.5, 'offence': 0.0, 'severity': 1.0, 'bodypart': 0.5, 'offence_severity': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 11.5540 | Val Accuracies: {'action': 0.0, 'offence': 0.5, 'severity': 0.5, 'bodypart': 0.0, 'offence_severity': 1.0}\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.4611 | Train Accuracies: {'action': 0.5, 'offence': 0.0, 'severity': 1.0, 'bodypart': 0.5, 'offence_severity': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 11.0010 | Val Accuracies: {'action': 0.0, 'offence': 0.5, 'severity': 0.5, 'bodypart': 1.0, 'offence_severity': 0.5}\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.3813 | Train Accuracies: {'action': 0.5, 'offence': 0.0, 'severity': 1.0, 'bodypart': 0.5, 'offence_severity': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 10.0522 | Val Accuracies: {'action': 0.5, 'offence': 0.5, 'severity': 0.5, 'bodypart': 1.0, 'offence_severity': 0.5}\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.7819 | Train Accuracies: {'action': 1.0, 'offence': 0.0, 'severity': 1.0, 'bodypart': 0.5, 'offence_severity': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 9.0070 | Val Accuracies: {'action': 0.5, 'offence': 0.5, 'severity': 0.5, 'bodypart': 1.0, 'offence_severity': 0.5}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "#from model import ImprovedTwoStreamNetwork\n",
    "from preprocess import load_filtered_clips_and_labels\n",
    "\n",
    "# Import your model\n",
    "#from model import TwoStreamNetwork  # Assuming the model code is saved as model.py\n",
    "\n",
    "# Custom Dataset class\n",
    "class ActionDataset(Dataset):\n",
    "    def __init__(self, rgb_clips, flow_clips, labels, transform=None):\n",
    "        self.rgb_clips = rgb_clips\n",
    "        self.flow_clips = flow_clips\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_frames = self.rgb_clips[idx]\n",
    "        flow_frames = self.flow_clips[idx]\n",
    "\n",
    "        # Apply transformation\n",
    "        if self.transform:\n",
    "            rgb_frames = [self.transform(frame) if not isinstance(frame, torch.Tensor) else frame for frame in rgb_frames]\n",
    "            flow_frames = [self.transform(frame) if not isinstance(frame, torch.Tensor) else frame for frame in flow_frames]\n",
    "\n",
    "        # Ensure dimensions are [num_frames, channels, height, width]\n",
    "        rgb_frames = torch.stack(rgb_frames, dim=0)\n",
    "        flow_frames = torch.stack(flow_frames, dim=0)\n",
    "\n",
    "        label_dict = {key: torch.tensor(self.labels[key][idx]) for key in self.labels.keys()}\n",
    "\n",
    "        return rgb_frames, flow_frames, label_dict\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = {key: [] for key in ['action', 'offence', 'severity', 'bodypart', 'offence_severity']}\n",
    "    all_labels = {key: [] for key in all_preds.keys()}\n",
    "\n",
    "    for rgb_input, flow_input, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Check input shapes and move to device\n",
    "        rgb_input, flow_input = rgb_input.to(device), flow_input.to(device)\n",
    "\n",
    "        # Verify dimensions; if missing batch dim, add it\n",
    "        if len(rgb_input.shape) == 4:\n",
    "            rgb_input = rgb_input.unsqueeze(0)  # Add batch dim if missing\n",
    "        if len(flow_input.shape) == 4:\n",
    "            flow_input = flow_input.unsqueeze(0)\n",
    "\n",
    "        labels = {key: val.to(device) for key, val in labels.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(rgb_input, flow_input)\n",
    "\n",
    "        # Compute losses for each task\n",
    "        loss = 0.0\n",
    "        for i, task in enumerate(all_preds.keys()):\n",
    "            task_loss = criterion(outputs[i], labels[task])\n",
    "            loss += task_loss\n",
    "            all_preds[task].extend(outputs[i].argmax(dim=1).cpu().numpy())\n",
    "            all_labels[task].extend(labels[task].cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = {task: accuracy_score(all_labels[task], all_preds[task]) for task in all_preds.keys()}\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = {key: [] for key in ['action', 'offence', 'severity', 'bodypart', 'offence_severity']}\n",
    "    all_labels = {key: [] for key in all_preds.keys()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rgb_input, flow_input, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            rgb_input, flow_input = rgb_input.to(device), flow_input.to(device)\n",
    "            labels = {key: val.to(device) for key, val in labels.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(rgb_input, flow_input)\n",
    "\n",
    "            # Compute losses and predictions for each task\n",
    "            loss = 0.0\n",
    "            for i, task in enumerate(all_preds.keys()):\n",
    "                task_loss = criterion(outputs[i], labels[task])\n",
    "                loss += task_loss\n",
    "                all_preds[task].extend(outputs[i].argmax(dim=1).cpu().numpy())\n",
    "                all_labels[task].extend(labels[task].cpu().numpy())\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = {task: accuracy_score(all_labels[task], all_preds[task]) for task in all_preds.keys()}\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def main(data_path, num_epochs=10, batch_size=1, learning_rate=1e-4, max_samples_o=1, max_samples_no =1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load data\n",
    "    train_rgb_clips, train_flow_clips, train_labels_action, train_labels_offence, train_labels_severity, train_labels_bodypart, train_labels_offence_severity = \\\n",
    "        load_filtered_clips_and_labels(data_path, \"train\", max_samples_o, max_samples_no)\n",
    "    \n",
    "    valid_rgb_clips, valid_flow_clips, valid_labels_action, valid_labels_offence, valid_labels_severity, valid_labels_bodypart, valid_labels_offence_severity = \\\n",
    "        load_filtered_clips_and_labels(data_path, \"valid\", max_samples_o, max_samples_no)\n",
    "\n",
    "    # Organize labels in a dictionary format\n",
    "    train_labels = {\n",
    "        \"action\": train_labels_action,\n",
    "        \"offence\": train_labels_offence,\n",
    "        \"severity\": train_labels_severity,\n",
    "        \"bodypart\": train_labels_bodypart,\n",
    "        \"offence_severity\": train_labels_offence_severity\n",
    "    }\n",
    "    valid_labels = {\n",
    "        \"action\": valid_labels_action,\n",
    "        \"offence\": valid_labels_offence,\n",
    "        \"severity\": valid_labels_severity,\n",
    "        \"bodypart\": valid_labels_bodypart,\n",
    "        \"offence_severity\": valid_labels_offence_severity\n",
    "    }\n",
    "\n",
    "    # Define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = ActionDataset(train_rgb_clips, train_flow_clips, train_labels, transform=transform)\n",
    "    valid_dataset = ActionDataset(valid_rgb_clips, valid_flow_clips, valid_labels, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = ImprovedTwoStreamNetwork().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training and validation loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Accuracies: {train_accuracy}\")\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_accuracy = validate(model, valid_loader, criterion, device)\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Accuracies: {val_accuracy}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"final_model.pth\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path with your actual data path\n",
    "    DATA_PATH = 'mvfouls'\n",
    "    main(data_path=DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
